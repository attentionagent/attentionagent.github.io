<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>

  <meta name="theme-color" content="#ffffff" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-158636121-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-158636121-1');
  </script>


  <!-- SEO -->
  <meta property="og:title" content="Neuroevolution of Self-Interpretable Agents" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Evolved to focus on a fraction of its vision critical for survival." />
  <meta property="og:image" content="https://attentionagent.github.io/assets/card/attention_agent_card_rect.png" />
  <meta property="og:url" content="https://attentionagent.github.io/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Neuroevolution of Self-Interpretable Agents" />
  <meta name="twitter:description" content="Evolved to focus on a fraction of its vision critical for survival." />
  <meta property="og:site_name" content="Neuroevolution of Self-Interpretable Agents" />
  <meta name="twitter:image" content="https://attentionagent.github.io/assets/card/attention_agent_card_square.png" />

</head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "Deep Neuroevolution of Self-Interpretable Agents"
  description: ""
</script>
<body>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<!--<td style="width: 50%;border: 1px solid transparent;"><video class="b-lazy" data-src="assets/mp4/carracing_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video></td>
<td style="width: 50%;border: 1px solid transparent;"><video class="b-lazy" data-src="assets/mp4/takecover_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video></td>-->
<td style="width: 50%;border: 0px solid transparent;"><video src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video></td>
<td style="width: 50%;border: 0px solid transparent;"><video src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video></td>
</tr></table>
<!--
<video class="b-lazy" data-src="assets/mp4/carracing_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 50%;" ></video>
<video class="b-lazy" data-src="assets/mp4/takecover_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 50%;" ></video>
-->

</div>

<dt-article id="dtbody">

<div style="text-align: center;">
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;"><br/>Examples of evolved self-attention agents</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
In this work, we evolve agents that attend to a small fraction of its visual input critical for its survival, allowing for interpretable agents that are not only compact, but also more generalizable. Here, we show examples of our agent's attention highlighted in white patches. In CarRacing (left), our agent mostly attends to the road borders, but shifts its focus to the turns before it changes heading directions. In DoomTakeCover (right), The agent is able to focus on fireballs and monsters, consistent with our intuitions.<br/>
</figcaption>
</div>
<!--In this work, we evolve agents that attend to a small fraction of its visual input critical for its survival, allowing for interpretable agents that are not only compact, but also more generalizable. Here, we show examples of our agent's attention highlighted in white patches. Note that our agent's decision-making controller receives <i>only</i> the <i>positions</i> of these white patches as visualized here, and <i>not</i> their contents. In CarRacing (left), our agent mostly attends to the road borders, but shifts its focus to the turns before it changes heading directions. In DoomTakeCover (right), The agent is able to focus on fireballs and monsters, consistent with our intuitions.<br/>-->

<!--<h2>‚ö†Ô∏è&nbsp;Draft. Please do not share this article‚ùóÔ∏èüôèüèºüôáüèª</h2>-->

<dt-byline class="l-page transparent"></dt-byline>
<h1>Neuroevolution of Self-Interpretable Agents</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://twitter.com/yujin_tang">Yujin Tang</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
    <div class="author">
        <a class="name" href="https://twitter.com/nt_duong">Duong Nguyen</a>
        <a class="affiliation" href="https://twitter.com/googlejapan">Google Japan</a>
    </div>
    <div class="author">
        <a class="name" href="https://twitter.com/hardmaru/">David Ha</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
  </div>
  <div class="date">
    <div class="month">March 18</div>
    <div class="year">2020</div>
  </div>
  <div class="date">
    <div class="month">YouTube</div>
    <div class="year"><a href="https://youtu.be/DTgeEWK_Wm0" target="_blank">Talk</a></div>
  </div>
  <div class="date">
    <div class="month">GECCO 2020<dt-fn>This work was presented at The Genetic and Evolutionary Computation Conference (<a href="https://gecco-2020.sigevo.org/index.html/HomePage" target="_blank">GECCO 2020</a>) as a full paper. It received the Best Paper Award in the EML Track.</dt-fn></div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/2003.08165" target="_blank">paper</a></div>
  </div>
</div>
</dt-byline>
<h2>Abstract</h2>
<p>Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight.
It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details.
Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a <em>self-attention</em> bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space.
We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent.
We argue that self-attention has similar properties as <em>indirect encoding</em>, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods.
Since our agent attends to only task-critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail.</p>
<hr>
<h2>Introduction</h2>
<p>There is much discussion in the deep learning community about the generalization properties of large neural networks. While larger neural networks generalize better than smaller networks, the reason is not that they have more weight parameters, but as recent work (e.g. <dt-cite key="frankle2018the,zhou2019deconstructing,ramanujan2019s"></dt-cite>) suggests, it is because larger networks allow the optimization algorithm to find good solutions, or <em>lottery tickets</em> <dt-cite key="frankle2018the"></dt-cite>, within a small fraction of the allowable solution space. These solutions can then be pruned to form sub-networks with useful <em>inductive biases</em> that have desirable generalization properties.</p>
<p>Recent neuroscience critiques of deep learning (e.g. <dt-cite key="zador2019critique,wann2019,hasson2020direct"></dt-cite>) point out that <em>animals are born with highly structured brain connectivity</em> that are <em>far too complex to be specified explicitly in the genome</em> and <em>must be compressed through a ‚Äúgenomic bottleneck‚Äù</em>--information encoded into the genome that specify a set of rules for wiring up a brain <dt-cite key="zador2019critique"></dt-cite>. Innate processes and behaviors are encoded by evolution into the genome, and as such <em>many of the neuronal circuits in animal brains are pre-wired, and ready to operate from birth <dt-cite key="hasson2020direct"></dt-cite>.</em> These innate abilities make it easier for animals to generalize and quickly adapt to different environments <dt-cite key="beyret2019animal"></dt-cite>.</p>
<p>There is actually a whole area of related research within the neurevolution field on evolving this <em>genetic bottleneck</em>, which is called an <em>indirect encoding</em>. Analogous to the pruning of lottery ticket solutions, indirect encoding methods allow for both the expressiveness of large neural architectures while minimizing the number of free model parameters. We believe that the foundations laid by the work on indirect encoding can help us gain a better understanding of the <em>inductive biases</em><dt-fn>The <em>inductive bias</em> of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not yet encountered. (<a href="https://en.wikipedia.org/wiki/Inductive_bias">Wikipedia</a>)</dt-fn> of neural networks and possibly offer a fresh perspective for approaching out-of-domain generalization problems.</p>
<!-- and possibly offer a fresh perspective for approaching generalization problems posed by the RL research community.

<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/pineau2018.mp4" type="video/mp4" controls playsinline style="width: 95%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Joelle Pineau's <dt-cite key="pineau2018">invited talk</dt-cite> at NeurIPS2018 drew our attention to the fact that humans can play Atari games even in the presence of much distraction to the game's visuals, and encouraged the AI research community to explore these challenges. In this work, our aim is to convince you that self-attention and neuroevolution are encouraging stepping stones for this direction of research. (Click Video to Play.)<br/>
</figcaption>
</div>-->
<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/hyperneat_example.jpeg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
HyperNEAT <dt-cite key="stanley2009hyperneat"></dt-cite> is an indirect encoding method that can generate a variety of weight patterns with structured regularities. Here are examples (taken from <dt-cite key="clune2011performance"></dt-cite>) of fully connected networks (<i>phenotype</i>) where the weights are not individually trained, but generated by a smaller neural network (<i>genotype</i>). The weight of each connection is a function of its location within the network. The thickness represents the magnitude of its weight.<br/>
</figcaption>
</div>
<p>Most current methods used to train neural networks, whether with gradient descent or evolution strategies, aim to solve for the value of each individual weight parameter of a given neural network. We refer to these methods as <em>direct encoding</em> methods.
<em>Indirect encoding</em> <dt-cite key="stanley2003taxonomy,schmidhuber1997discovering"></dt-cite>, on the other hand, offers a radically different approach. These methods optimize instead for a <em>small</em> set of rules or operations, referred to as the <em>genotype</em>, that specify how the (much larger) neural network (the <em>phenotype</em>) should be generated.<dt-fn>These terms in the evolutionary computing literature were taken from the evolutionary biology field, where the <em>genotype</em> is the part of the genetic makeup of a cell, and therefore of any individual organism, which determines the organism's actual observed properties, such as morphology, development, or behavior (the <em>phenotype</em>) (<a href="https://en.wikipedia.org/wiki/Genotype%E2%80%93phenotype_distinction">Wikipedia</a>)</dt-fn>
In general, the phenotype encompasses both the neural architecture and its weights, but contemporary indirect encoding methods (e.g. <dt-cite key="stanley2009hyperneat,koutnik2013evolving"></dt-cite>) typically generate only the weights of a pre-defined architecture using a small set of genotype parameters.</p>
<p>Before the popularity of Deep RL, indirect encoding methods in the neuroevolution literature have been a promising approach for the types of problems that eventually used Deep RL solutions.
In the case of vision-based RL problems, earlier works demonstrated that large neural networks can be encoded into much smaller, genotype solutions, that are capable of playing Atari from pixels <dt-cite key="hausknecht2012hyperneat"></dt-cite> (when it was still considered challenging in 2012) or car racing directly from pixel-only inputs <dt-cite key="koutnik2013evolving"></dt-cite>, hinting at the potential power of indirect encoding.
Even before deep learning and convolutional networks started to gain traction in 2012 <dt-cite key="krizhevsky2012imagenet"></dt-cite>, indirect encoding has enabled neural network controllers to play board games with structural regularities such as Checkers <dt-cite key="gauci2008case"></dt-cite> and Go <dt-cite key="gauci2010indirect"></dt-cite>.</p>
<p>By encoding the weights of a large model with a small number of parameters, we can substantially reduce the search space of the solution, at the expense of restricting our solution to a small subspace of all possible solutions offered by direct encoding methods. This constraint naturally incorporates into our agent an inductive bias that determines what it does well at <dt-cite key="wann2019,zador2019critique,hasson2020direct"></dt-cite>, and this inductive bias is dependent on the choice of our indirect encoding method. For instance, HyperNEAT <dt-cite key="stanley2009hyperneat"></dt-cite> has been successful at robotic gait control <dt-cite key="clune2009evolving,risi2013confronting"></dt-cite>, suggesting CPPNs<dt-fn>Compositional pattern-producing network (CPPN) is the genotype in the HyperNEAT‚Å† algorithm‚Äîa small neural network that produces patterns and regularities that define the weights of a larger phenotype network. (<a href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network">Wikipedia</a>)</dt-fn> <dt-cite key="stanley2007cppn"></dt-cite> to be effective at representing modular and symmetric properties suitable for locomotion. But are there indirect encoding methods that are better suited for vision-based RL tasks?</p>
<div style="text-align: center;">
<figcaption style="text-align: left; color:#000; padding-top: 0;">Car Racing environment</figcaption>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_stages.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; color:#000; padding-top: 0;">Take Cover environment</figcaption>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_stages.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Perceiving the world through a self-attention bottleneck</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
Our agent receives visual input as a stream of 96x96px RGB images (<i>left</i>). Each image frame is passed through a self-attention bottleneck module, responsible for selecting K=10 patches (highlighted in white, <i>middle</i>). Features from these K patches (such as location) are then routed to a decision-making controller (<i>right</i>) that will produce the agent's next action. The parameters of the self-attention module and the controller are trained together using neuroevolution.<br/>
</figcaption>
</div>
<p>In this work, we establish that <em>self-attention</em> can be viewed as a form of indirect encoding, which enables us to construct highly parameter-efficient agents.
We investigate the performance and generalization properties of these agents for vision-based RL tasks.
Self-attention has been popularized by Transformer <dt-cite key="NIPS2017_7181"></dt-cite> models that have been successfully applied in domains such as natural language processing <dt-cite key="devlin-etal-2019-bert,radford2019language"></dt-cite> and vision <dt-cite key="DBLP:journals/corr/abs-1911-03584,DBLP:conf/nips/ParmarRVBLS19,hu2019local,bello2019attention"></dt-cite>. As we will explain,
self-attention offers a simple yet powerful approach for parameterizing a large weight matrix of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">O</mi></mrow><mo>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose">)</span></span></span></span> using only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">O</mi></mrow><mo>(</mo><mi>d</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mclose">)</span></span></span></span> number of parameter values, where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> is the size of the visual input, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span></span></span></span> is the dimension of some transformed space and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>‚â´</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n \gg d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">‚â´</span><span class="mord mathit">d</span></span></span></span>.
Furthermore, such a parameterization enforces an inductive bias to encourage our agent to attend to only a small fraction of its visual input, and as such naturally makes the agent more interpretable.</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mods.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 98.5%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Environments outside of training distribution for Take Cover</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
In psychology, <dt-cite key="mack1998inattentional">inattentive blindness</dt-cite> results from a lack of attention that is not associated with vision defects, as individuals fail to perceive an unexpected stimulus in plain sight. Mack and Rock<dt-cite key="mack1998inattentional"></dt-cite> make the radical claim <i>that there is no conscious perception of the visual world without attention to it</i>. In this work, we model this phenomenon in simulation and restrict our agent's decision-making controller access only a small faction of its visual input. We demonstrate that an agent's <i>inattentive blindness</i> can result in better generalization, simply due to its ability to ‚Äúnot see things‚Äù that can confuse it. Here, our agent is evolved to survive in the top-left environment only, but it can also survive in unseen settings with higher walls, different floor textures, or when confronted with a distracting sign.<br/>
</figcaption>
</div>
<p>As we will show, neuroevolution is an ideal method for training self-attention agents, because not only can we remove unnecessary complexity required for gradient-based methods, resulting in much simpler architectures, we can also incorporate modules that enhance the effectiveness of self-attention that need not be differentiable. We showcase self-attention agents trained with neuroevolution that require 1000x fewer parameters than conventional methods and yet is able to solve challenging vision-based RL tasks. Specifically, with less than 4000 parameters, our self-attention agents can reach average scores of 914 over 100 consecutive trials in a 2D car racing task <dt-cite key="brockman2016openai"></dt-cite> and 1125 in a 3D VizDoom task <dt-cite key="DBLP:journals/tciaig/WydmuchKJ19"></dt-cite> (the tasks are considered solved for scores &gt;900 and &gt;750), comparable with existing state-of-the-art (SOTA) results in <dt-cite key="ha2018worldmodels,DBLP:conf/gecco/RisiS19,DBLP:journals/corr/abs-2001-01683"></dt-cite>. Moreover, our agent learns to attend to only task-critical visual spots and is therefore able to generalize to environments where task irrelevant elements are modified whereas conventional methods fail.</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_mods.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 98.5%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Environments outside of training distribution for Car Racing</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
Humans who learn to drive during a sunny day can (to some extent) also drive at night, on a rainy day, in a different car, or in the presence of bird droppings on the windshield. Our self-attention agent is not only able to solve CarRacing-v0<dt-cite key="CarRacing-v0"></dt-cite>, achieving an average score > 900, it can also achieve similar performance in unseen conditions (such as brighter or darker scenery, or having its vision modified by artifacts such as side bars or background blobs), while requiring 1000x fewer parameters than conventional methods that fail to generalize.<br/>
</figcaption>
</div>
<p>The goal of this work is to showcase self-attention as a powerful tool for the neuroevolution toolbox, and we will open-source code for reproducing our experiments. We hope our results will encourage further investigation into the neuroevolution of self-attention models, and also revitalize interest in indirect encoding methods.</p>
<hr>
<h2>Background on Self-Attention</h2>
<p>We now give a brief overview of self-attention. Here, we describe a simpler subset of the full Transformer <dt-cite key="NIPS2017_7181"></dt-cite> architecture used in this work. In particular, we omit <em>Value</em> matrices, <em>positional encoding</em>, <em>multi-head attention</em> from our method, and opt for the simplest variation that complements neuroevolution methods for our purpose.
We refer to <dt-cite key="bloem2018"></dt-cite> for an in-depth overview of the Transformer model.</p>
<p>Let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>‚àà</mo><mspace width="0.277778em"></mspace><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">X \in\;\mathcal{R}^{n \times d_{in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.888208em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mrel">‚àà</span><span class="mord mspace thickspace"></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mbin">√ó</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> be an input sequence of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> elements (e.g. number of words in a sentence, pixels in an image), each of dimensions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> (e.g. word embedding size, RGB intensities).
Self-attention module calculates an <em>attention score matrix</em> and a weighted output:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mrow><mi>A</mi></mrow></mtd><mtd><mrow><mrow></mrow><mo>=</mo><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext><mo fence="false">(</mo><mfrac><mrow><mn>1</mn></mrow><mrow><msqrt><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msqrt></mrow></mfrac><mo>(</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub><mo>)</mo><mo>(</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub><msup><mo>)</mo><mo>‚ä∫</mo></msup><mo fence="false">)</mo></mrow></mtd><mtd><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mrow><mi>Y</mi></mrow></mtd><mtd><mrow><mrow></mrow><mo>=</mo><mi>A</mi><mi>X</mi></mrow></mtd><mtd><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{aligned}
   A&amp;=\text{softmax}\big(\frac{1}{\sqrt{d_{in}}}(X W_k) (X W_q)^\intercal \big) &amp; (1) \\
   Y&amp;=A X &amp; (2)
\end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.9757200000000001em;"></span><span class="strut bottom" style="height:3.4514400000000007em;vertical-align:-1.4757200000000006em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist"><span style="top:-0.6542800000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit">A</span></span></span><span style="top:1.1157200000000005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">Y</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="col-align-l"><span class="vlist"><span style="top:-0.6542800000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"></span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mord"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size1">(</span></span></span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.7472200000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">‚àö</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span>‚Äã</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord amsrm">‚ä∫</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mord"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size1">)</span></span></span></span></span><span style="top:1.1157200000000005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"></span><span class="mrel">=</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span>‚Äã</span></span></span><span class="arraycolsep" style="width:2em;"></span><span class="col-align-r"><span class="vlist"><span style="top:-0.6542800000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="mord displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span><span style="top:1.1157200000000005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="mord displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>q</mi></msub><mo>‚àà</mo><mspace width="0.277778em"></mspace><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_k, W_q \in\;\mathcal{R}^{d_{in} \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.135216em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">‚àà</span><span class="mord mspace thickspace"></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mbin">√ó</span><span class="mord mathit">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> are matrices that map the input to components called <em>Key</em> and <em>Query</em> (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mtext><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><mtext><mi mathvariant="normal">Q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mtext><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\text{Key} = XW_k, \text{Query} = XW_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="text mord textstyle uncramped"><span class="mord mathrm">K</span><span class="mord mathrm">e</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mpunct">,</span><span class="text mord textstyle uncramped"><span class="mord mathrm">Q</span><span class="mord mathrm">u</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span>), <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span></span></span></span> is the dimension of the transformed space and is usually a small integer.
Since the average value of the dot product grows with the vector's dimension, each entry in the Key and Query matrices can be disproportionally too large if <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> is large. To counter this, the factor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><msqrt><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\sqrt{d_{in}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.695108em;vertical-align:-0.8500000000000001em;"></span><span class="base textstyle uncramped"><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.5855539999999999em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:0.09206571428571442em;"><span class="style-wrap reset-scriptstyle textstyle uncramped">‚àö</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1.4285714285714286em;">‚Äã</span></span><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span><span style="top:-0.9936485714285714em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1.4285714285714286em;">‚Äã</span></span><span class="reset-scriptstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1.4285714285714286em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">‚Äã</span></span>‚Äã</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span> is used to normalize the inputs.
Applying the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext></mrow><annotation encoding="application/x-tex">\text{softmax}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="text mord textstyle uncramped"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span><dt-fn><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>=</mo><mi>exp</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mi mathvariant="normal">/</mi><msub><mo>‚àë</mo><mrow><mi>k</mi></mrow></msub><mrow><mi>exp</mi><mo>(</mo><msub><mi>x</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{softmax}(x_i) = \exp(x_i) / \sum_{k}{\exp(x_k)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.0500099999999999em;vertical-align:-0.30001em;"></span><span class="base textstyle uncramped"><span class="text mord textstyle uncramped"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose">)</span><span class="mord mathrm">/</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">‚àë</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mord textstyle uncramped"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose">)</span></span></span></span></span></dt-fn> operation along the rows of the matrix product in Equation 1, we get the attention matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mo>‚àà</mo><mspace width="0.277778em"></mspace><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>n</mi><mo>√ó</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in\;\mathcal{R}^{n \times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:0.810431em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span><span class="mrel">‚àà</span><span class="mord mspace thickspace"></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mbin">√ó</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span>, where each row vector of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> sums to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span></span></span></span>.
Thus, each row of output <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><mo>‚àà</mo><mspace width="0.277778em"></mspace><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">Y \in\;\mathcal{R}^{n \times d_{in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.888208em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">Y</span><span class="mrel">‚àà</span><span class="mord mspace thickspace"></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mbin">√ó</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> can be interpreted as a weighted average of the input <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span> by each row of the attention matrix.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/annotated_transformer_01.png" style="display: block; margin: auto; width: 95%;"/>
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/annotated_transformer_02.png" style="display: block; margin: auto; width: 95%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<b>Visualizing Attention Layers </b> from the Transfomer model. Figures from <dt-cite key="opennmt">The Annotated Transformer</dt-cite>.<br/>
</figcaption>
</div>
<p>Self-attention lets us map arbitrary input <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span> to target output <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">Y</span></span></span></span>, and this mapping is determined by an attention matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> parameterized by much smaller Key and Query parameters, which can be trained using machine learning techniques. The self-attention mechanism is at the heart of recent SOTA methods for translation and language modeling <dt-cite key="devlin-etal-2019-bert,radford2019language"></dt-cite>, and has now become a common place method for natural language processing domain.</p>
<h3>Self-Attention for Images</h3>
<p>Although self-attention is broadly applied to sequential data, it is straightforward to adapt it to images.
For images, the input is a tensor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>‚àà</mo><mspace width="0.277778em"></mspace><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>H</mi><mo>√ó</mo><mi>W</mi><mo>√ó</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in\;\mathcal{R}^{H \times W \times C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.880431em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mrel">‚àà</span><span class="mord mspace thickspace"></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.08125em;">H</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.08125em;">H</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">W</span></span></span></span> are the height and width of the image, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> is the number of image channels (e.g., 3 for RGB, 1 for gray-scale).
If we reshape the image so that it becomes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>‚àà</mo><mspace width="0.277778em"></mspace><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">X \in\;\mathcal{R}^{n \times d_{in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.888208em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mrel">‚àà</span><span class="mord mspace thickspace"></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mbin">√ó</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><mi>H</mi><mo>√ó</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">n = H \times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.08125em;">H</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.13889em;">W</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">d_{in} = C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span>, all the operations defined in Equations 1-2 are valid and can be readily applied.
In the reshaped <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span>, each row represents a pixel and the attentions are between pixels. Notice that the complexity of Equation 1 grows quadratically with the number of rows in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span> due to matrix multiplication, it therefore becomes computationally prohibitive when the input image is large.
While down-sampling the image before applying self-attention is a quick fix, it is accompanied with performance trade-off. For more discussion and methods to partially overcome this trade-off for images, see <dt-cite key="DBLP:conf/nips/ParmarRVBLS19,DBLP:journals/corr/abs-1911-03584"></dt-cite>.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/algo_flow01.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Rather than applying self-attention on individual pixels, we extract patches from an input image using a slide window. We will explain the approach in greater detail later on in the <i>Self-Attention Agent</i> section.<br/>
</figcaption>
</div>
<p>Instead of applying operations on individual pixels of the entire input, a popular method for image processing is to organize the image into patches and take them as inputs as described in previous work (e.g. <dt-cite key="DBLP:journals/corr/abs-1905-02793,Ding2019ImprovingSS"></dt-cite><dt-cite key="DBLP:journals/corr/abs-1904-01784,NIPS2019_8359"></dt-cite>). In our approach, our agent attends to patches of the input rather than individual pixels, and we use a sliding window to crop the input image in our input transformations. Conceptually, our approach is similar to <em>Spatial Softmax</em> <dt-cite key="finn2016deep,suwajanakorn2018discovery,kulkarni2019unsupervised"></dt-cite>, which compresses visual inputs into a set of 2D keypoints that are relevant to the task.
This has been shown to work on robot perception tasks, and in some cases the keypoints are spatially interpretable.</p>
<hr>
<h2>Self-Attention as a form of Indirect Encoding</h2>
<p>Indirect encoding methods represent the weights of a neural network, the <em>phenotype</em>, with a smaller set of <em>genotype</em> parameters. How a genotype encodes a larger solution space is defined by the indirect encoding algorithm. HyperNEAT <dt-cite key="stanley2009hyperneat"></dt-cite> encodes the weights of a large network via a coordinate-based CPPN-NEAT <dt-cite key="stanley2007cppn"></dt-cite> network, while Compressed Network Search <dt-cite key="koutnik2013evolving"></dt-cite> uses discrete cosine transform to compress the weights of a large weight matrix into a small number of DCT coefficients, similar to JPEG compression. Examples of patterns produced with these methods:</p>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 33.3%;border: 1px solid transparent;">
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/synesthesia.jpeg" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/synesthesia.jpeg" style="display: block; margin: auto; width: 95%;"/></a>
</td>
<td style="width: 33.3%;border: 1px solid transparent;">
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/nuclear_fusion.jpeg" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/nuclear_fusion.jpeg" style="display: block; margin: auto; width: 95%;"/></a>
</td>
<td style="width: 33.3%;border: 1px solid transparent;">
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/radioactive_frog.jpeg" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/radioactive_frog.jpeg" style="display: block; margin: auto; width: 95%;"/></a>
</td>
</tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
<b>Patterns produced using CPPN-NEAT <dt-cite key="stanley2007cppn,otorogallery"></dt-cite>.</b>  Rich pattern-generators that produce structured regularities can be used in <i>indirect encoding</i> methods to produce weight values of large neural networks, as demonstrated in HyperNEAT <dt-cite key="stanley2009hyperneat"></dt-cite>. (Click Image to Enlarge)<br/>
</figcaption>
</div>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 33.3%;border: 1px solid transparent;">
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/cns01.jpeg" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/cns01.jpeg" style="display: block; margin: auto; width: 95%;"/></a>
</td>
<td style="width: 33.3%;border: 1px solid transparent;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/cns02.png" style="display: block; margin: auto; width: 95%;"/>
</td>
<td style="width: 33.3%;border: 1px solid transparent;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/cns03.png" style="display: block; margin: auto; width: 95%;"/>
</td>
</tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
<b>Compressed Network Search <dt-cite key="koutnik2013evolving"></dt-cite></b> uses Discrete Cosine Transform to generate a large number of feature detectors using only a small number of learnable coefficients. The patterns generated also exhibit regular patterns useful for continuous control tasks. (Click Image to Enlarge)<br/>
</figcaption>
</div>
<p>Due to compression, the space of possible weights an indirect encoding scheme can produce is only a small subspace of all possible combination of weights. The constraint on the solution space resulting from indirect encoding enforces an inductive bias into the phenotype. While this bias determines the types of tasks that the network is <em>naturally</em> suited at doing, it also restricts the network to a subset of all possible tasks that an unconstrained phenotype can (in theory) perform. More recent works have proposed ways to broaden its task domain of indirect encoding. ES-HyperNEAT <dt-cite key="risi2012enhanced"></dt-cite> proposed adapting part of the indirect encoding algorithm itself to the task environment. Hypernetworks <dt-cite key="ha2017hypernetworks"></dt-cite> suggested making the phenotype directly dependent on the inputs, thus tailoring the weights of the phenotype to the specific inputs of the network. Following this approach of incorporating information from the input into the weight-generating process, it has been shown <dt-cite key="munkhdalai2017meta,chen2018neural"></dt-cite><dt-cite key="dumoulin2018featurewise,Oswald2020Continual"></dt-cite> that the phenotype can be highly expressive as the weights can adapt to the inputs for the task at hand, while static indirect encoding methods cannot.</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/hypernetworks_example.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%; padding-top: 0; padding-bottom: 0;" ></video>
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/hypernetworks_example.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<br/><b>Hypernetworks</b> <dt-cite key="ha2017hypernetworks"></dt-cite> is an example of a <i>dynamic</i> indirect encoding method where the weights produced are allowed to change over time. When we train a hypernetwork model to generate synthetic handwriting or text, the model samples an action from a probability distribution conditioned on its internal state, and performs the action. But unlike conventional RNNs, its weights (represented by the four colors) can change over time, and is dependent on the agent's internal state and inputs (its previous action). Self-attention is also a form of dynamic indirect encoding as the weights produced are also dependent on the agent's senses and will vary over time.<br/>
</figcaption>
</div>
<p>Similarly, self-attention enforces a structure on the attention weight matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> in Equation 1 that makes it also input-dependent. If we remove the Key and Query terms, the outer product <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">X X^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> defines an association matrix <dt-fn><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">X X^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> are also known as Gram matrices, and are key to all of kernel methods and classical statistical learning. (<a href="https://en.wikipedia.org/wiki/Gramian_matrix">Wikipedia</a>)</dt-fn> where the elements are large when two distinct input terms are in agreement. This type of structure enforced in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> has been shown to be suited for associative tasks where the downstream agent has to learn the relationship between unrelated items. For example, they are used in the Hebbian learning <dt-cite key="hebb1949organization"></dt-cite> rule inspired by <em>neurons that fire together wire together</em>, shown to be useful for associative learning <dt-cite key="ba2016using,miconi2018differentiable"></dt-cite>. Matrix factorization applied to weights has been proposed in the deep learning literature <dt-cite key="sainath2013low,grosse2016kronecker"></dt-cite>, and are also present in recommender systems <dt-cite key="koren2009matrix"></dt-cite> to represent relationships between different inputs.</p>
<div style="text-align: center;">
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/transformer_xl_01.png" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/transformer_xl_01.png" style="display: block; margin: auto; width: 95%;"/></a>
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/transformer_xl_02.png" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/examples/transformer_xl_02.png" style="display: block; margin: auto; width: 95%;"/></a>
<figcaption style="text-align: left; padding-top: 0;">
<b>Visualization of Large Attention Layers </b> from the <dt-cite key="dai2019transformer">Transfomer-XL</dt-cite> model. (Click Image to Enlarge)<br/>
</figcaption>
</div>
<p>As the outer product <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">X X^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> so far has no free parameters, the corresponding matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> will not be suitable for arbitrary tasks beyond association. The role of the small Key (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span>) and Query (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span>) matrices in Equation 1 allow <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> to be modified for the task at hand. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> can be viewed as the <em>genotype</em> of this indirect-encoding method.</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>q</mi></msub><mo>‚àà</mo><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_k, W_q \in \mathcal{R}^{d_{in} \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.135216em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">‚àà</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mbin">√ó</span><span class="mord mathit">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> are the matrices that contain the free parameters, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> is a constant with image inputs (3 for RGB images and 1 for gray scale images), therefore the number of free parameters in self-attention is in the order of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">O</mi></mrow><mo>(</mo><mi>d</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mclose">)</span></span></span></span>.
As we explained previously, when applying self-attention to images <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> can be the number of pixels in an input the magnitude of which is often tens of thousands even for small images (e.g. 100px √ó 100px). On the other hand, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span></span></span></span> is the dimension of the transformed space in which the Key and Query matrices reside and is often much smaller than <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">d=4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span><span class="mrel">=</span><span class="mord mathrm">4</span></span></span></span> in our experiments). This form of indirect encoding enables us to represent the phenotype, the attention matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span>, of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">O</mi></mrow><mo>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mclose">)</span></span></span></span> using only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">O</mi></mrow><mo>(</mo><mi>d</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mclose">)</span></span></span></span> number of genotype parameters. In our experiments, we show that our attention matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span> can be represented using only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>‚àº</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">‚àº</span></span></span></span> 1200 trainable genotype parameters.</p>
<p>Furthermore, we demonstrate that features from this attention matrix is especially useful to a downstream decision-making controller. We find that even if we restrict the size of our controller to only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>‚àº</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">‚àº</span></span></span></span> 2500 parameters, it can still solve challenging vision-based tasks by leveraging the information provided by self-attention.</p>
<hr>
<h2>Self-Attention Agent</h2>
<p>The design of our agent takes inspiration from concepts related to inattentive blindness <dt-cite key="mack1998inattentional"></dt-cite>--when the brain is involved in effort-demanding tasks, it assigns most of its attention capacity only to task relevant elements and is temporarily blind to other signals <dt-cite key="mack1998inattentional,kahneman2011thinking"></dt-cite>.
In this vein, our agent is designed to focus on only task-critical regions in the input image and will ignore the others.</p>
<p>The following figure depicts an overview of our self-attention agent:</p>
<div style="text-align: center;">
<br/>
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/algo_flow.png" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/algo_flow.png" style="display: block; margin: auto; width: 100%;"/></a>
<figcaption>
<b>Method overview.</b> Illustration of data processing flow in our self-attention agent. We will explain each module in greater detail later on in this section. (Click Image to Enlarge)<br/>
</figcaption>
</div>
<p>There are four stages of information processing:</p>
<p><strong>Input Transformation</strong>¬†Given an observation, our agent first resizes it into an input image of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>√ó</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">L \times L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">L</span><span class="mbin">√ó</span><span class="mord mathit">L</span></span></span></span>, the agent then segments the image into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> patches and regard each patch as a potential region to attend to.</p>
<p><strong>Importance Voting via Self-Attention</strong>¬†To decide which patches are appropriate, the agent passes the patches to the self-attention module to get a vector representing each patch's importance, based on which it selects <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patches of the highest importance.</p>
<p><strong>Patch Selection and Feature Retrieval</strong>¬†Our agent then uses the index (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span>) of each of the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patches to fetch relevant features of each patch with a function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span>, which can be either a learned module or a pre-defined function that incorporates domain knowledge.</p>
<p><strong>Controller</strong>¬†Finally, the agent inputs the features into its controller that outputs the action it will execute in its environment.</p>
<p>Each of these stages will be explained in greater detail in this section.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/hyperparam_count.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<br/><b>Hyper-parameters in this paper.</b> Left: Parameters for input transformation. After resizing the observation into an image of shape L √ó L, we use a sliding window of specified size and stride to segment the image into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>=</mo><mo>(</mo><mrow><mo fence="true">‚åä</mo><mfrac><mrow><mi>L</mi><mo>‚àí</mo><mi>M</mi></mrow><mrow><mi>S</mi></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">‚åã</mo></mrow><msup><mo>)</mo><mn>2</mn></msup><mo>=</mo><mo>(</mo><mrow><mo fence="true">‚åä</mo><mfrac><mrow><mn>9</mn><mn>6</mn><mo>‚àí</mo><mn>7</mn></mrow><mrow><mn>4</mn></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">‚åã</mo></mrow><msup><mo>)</mo><mn>2</mn></msup><mo>=</mo><mn>5</mn><mn>2</mn><mn>9</mn></mrow><annotation encoding="application/x-tex">N = (\left \lfloor \frac{L-M}{S} + 1 \right \rfloor)^2 = (\left \lfloor \frac{96 - 7}{4} + 1 \right \rfloor)^2 = 529</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.872331em;"></span><span class="strut bottom" style="height:1.222341em;vertical-align:-0.35001em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mrel">=</span><span class="mopen">(</span><span class="minner textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">‚åä</span></span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">L</span><span class="mbin">‚àí</span><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">‚åã</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">=</span><span class="mopen">(</span><span class="minner textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">‚åä</span></span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">4</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">9</span><span class="mord mathrm">6</span><span class="mbin">‚àí</span><span class="mord mathrm">7</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">‚åã</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">=</span><span class="mord mathrm">5</span><span class="mord mathrm">2</span><span class="mord mathrm">9</span></span></span></span> patches.
Right: Parameters for self-attention. Since the attention is between patches and each patch is RGB, we therefore have <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><msup><mi>M</mi><mn>2</mn></msup><mo>√ó</mo><mi>C</mi><mo>=</mo><msup><mn>7</mn><mn>2</mn></msup><mo>√ó</mo><mn>3</mn><mo>=</mo><mn>1</mn><mn>4</mn><mn>7</mn></mrow><annotation encoding="application/x-tex">d_{in} = M^2 \times C = 7^2 \times 3 = 147</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.964108em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mrel">=</span><span class="mord"><span class="mord mathrm">7</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mbin">√ó</span><span class="mord mathrm">3</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">4</span><span class="mord mathrm">7</span></span></span></span>.<br/>
</figcaption>
</div>
<p>To gain a better sense of the magnitudes involved, we summarize the hyper-parameters used in this work in the table above. Some of the parameters are explained in the following sections.</p>
<div style="text-align: center;">
<figcaption style="text-align: left; color:#000; padding-top: 0;">Car Racing environment</figcaption>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_stages.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; color:#000; padding-top: 0;">Take Cover environment</figcaption>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_stages.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Visualizing the various stages of information processing in action</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
<b>Input Transformation</b>&nbsp;Our agent's' visual input is resized to 96x96px RGB images, and divided up into patches. (<i>left</i>)<br/>
<b>Importance Voting via Self-Attention</b>&nbsp;This module votes for the top K=10 patches (highlighted in white). (<i>middle</i>)<br/>
<b>Patch Selection and Feature Retrieval</b>&nbsp;Features from these K patches (such as location) routed to <b>Controller</b>. (<i>right</i>)
</figcaption>
</div>
<h3>Input Transformation</h3>
<p>Our agent does some basic image processing and then segments an input image into multiple patches.
For all the experiments in this paper, our agent receives RGB images as its input, therefore we simply divide each pixel by 255 to normalize the data, but it should be straightforward to integrate other data preprocessing procedures.
Similarly, while there can be various methods for image segmentation, we find a simple sliding window strategy to be sufficient for the tasks in this work.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/algo_flow01.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
After some basic image pre-processing, we extract patches from an input image using a slide window of size M√óM and stride S. This results in an output shape of (N, M, M, C), where N is the number of patches, M is the height / width of each patch, and C is the number of channels in the input image. We then flatten the data.<br/>
</figcaption>
</div>
<p>To be concrete, when the window size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> and stride <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span> are specified, our agent chops an input of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo separator="true">,</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(H, W, C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span> into a batch of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> patches of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>M</mi><mo separator="true">,</mo><mi>M</mi><mo separator="true">,</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(M, M, C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.08125em;">H</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">W</span></span></span></span> are the height and width of the input image and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> is the number of channels.
We then reshape the processed data into a matrix of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>N</mi><mo separator="true">,</mo><mi>M</mi><mo>√ó</mo><mi>M</mi><mo>√ó</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(N, M \times M \times C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span> before feeding it to the self-attention module.
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span> are hyper-parameters to our model, they determine how large each patch is and whether patches overlap. In the extreme case when <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>=</mo><mi>S</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">M = S = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span> this becomes self-attention on each individual pixel in the image.</p>
<h3>Importance Voting via Self-Attention</h3>
<p>Upon receiving the transformed data in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{R}^{n \times d_{in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mbin">√ó</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">n=N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><mi>M</mi><mo>√ó</mo><mi>M</mi><mo>√ó</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">d_{in} = M \times M \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span>, the self-attention module follows Equation 1 to get the attention matrix of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>N</mi><mo separator="true">,</mo><mi>N</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(N, N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>.
To keep the agent as simple as possible, we do not use positional encoding in this work.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/algo_flow02.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
We project the flattened patches into Keys and Queries and calculate their attention matrix of shape (N, N). Softmax is applied along each row. We then sum the matrix along the rows to get a patch importance vector.<br/>
</figcaption>
</div>
<p>By applying softmax, each row in the attention matrix sums to one, so the attention matrix can be viewed as the results from a voting mechanism between the patches.
To be specific, if each patch can distribute fractions of a total of 1 vote to other patches (including itself), row <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span> thus shows how patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span> has voted and column <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span> gives the votes that patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span> acquired from others.
In this interpretation, entry <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i, j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span> in the attention matrix is then regarded as how important patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span> is from patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span>'s perspective.
Taking sums along the columns of the attention matrix results in a vector that summarizes the total votes acquired by each patch, and we call this vector the <em>patch importance vector</em>.
Unlike conventional self-attention, we rely solely on the patch importance vector and do not calculate a weighted output with Equation 2.</p>
<h3>Patch Selection and Feature Retrieval</h3>
<p>Based on the patch importance vector, our agent picks the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patches with the highest importance. We pass in the index of these <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patches (denoted as index <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span> to reference the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>k</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">k^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">h</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> patch) into a <em>feature retrieval operation</em> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> to query the for their features. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> can be static mappings or learnable modules, and it returns the features related to the image region centered at patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span>'s position. The following list gives examples of possible features:</p>
<ul>
<li>
<p><strong>Patch center position.</strong>¬†<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>:</mo><mrow><mi mathvariant="script">R</mi></mrow><mo>‚Ü¶</mo><msup><mrow><mi mathvariant="script">R</mi></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(k): \mathcal{R} \mapsto \mathcal{R}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mrel">:</span><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="mrel">‚Ü¶</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> where the output contains the row and column indices of patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span>'s center position. This is the plain and simple method that we use in this work.</p>
</li>
<li>
<p><strong>Patch's image histogram.</strong>¬†<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>:</mo><mrow><mi mathvariant="script">R</mi></mrow><mo>‚Ü¶</mo><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">f(k): \mathcal{R} \mapsto \mathcal{R}^{b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mrel">:</span><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="mrel">‚Ü¶</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">b</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> where the output is the image histogram calculated from patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span> is the number of bins.</p>
</li>
<li>
<p><strong>Convolution layers' output.</strong>¬†<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>:</mo><mrow><mi mathvariant="script">R</mi></mrow><mo>‚Ü¶</mo><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mi>s</mi><mo>√ó</mo><mi>s</mi><mo>√ó</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">f(k): \mathcal{R} \mapsto \mathcal{R}^{s \times s \times m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mrel">:</span><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="mrel">‚Ü¶</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">s</span><span class="mbin">√ó</span><span class="mord mathit">s</span><span class="mbin">√ó</span><span class="mord mathit">m</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> is a stack of convolution layers (learnable or fixed with pre-trained weights). It takes the image region centered at patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span> as input and outputs a tensor of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>√ó</mo><mi>s</mi><mo>√ó</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">s \times s \times m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mbin">√ó</span><span class="mord mathit">s</span><span class="mbin">√ó</span><span class="mord mathit">m</span></span></span></span>.</p>
</li>
</ul>
<p>The design choices of these features give us control over various aspects of the agent's capabilities, interpretability and computational efficiency.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/algo_flow03.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
We sort the patch importance vector and extract the indices of the top K patches that have the highest importance. We then map the patch indices to the corresponding features and input to the decision-making controller obtain the next action. Note that these types of <i>argsort and slice</i> operations (such as importance sorting and patch pruning described in this section) are inherently non-differentiable, motivating the use of neuroevolution methods such as CMA-ES <dt-cite key="Hansen2006"></dt-cite> as we will discuss later on.<br/>
</figcaption>
</div>
<p>By discarding patches of low importance the agent becomes temporarily blind to other signals, this is built upon our premise and effectively creates a bottleneck that forces the agent to focus on patches only if they are critical to the task.
Once learned, we can visualize the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patches and see directly what the agent is attending to.</p>
<!--Although this mechanism introduces $K$ as a hyper-parameter, by observing the input images we usually have good priors on the region of $K$ and is thus easy to tune compared to other optimization related hyper-parameters (this applies to $M$ and $S$ too).-->
<p>Although this mechanism introduces <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> as a hyper-parameter, we find it easy to tune (along with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span>). In principle we can also let neuroevolution decide on the number of patches, and we will leave this for future work.</p>
<p>Pruning less important patches also leads to the reduction of input features, so the agent is more efficient by solving tasks with fewer weights.
Furthermore, correlating the feature retrieval operation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> with individual patches can also lower the computational cost.
For instance, if some local features are known to be useful for the task yet computationally expensive, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> acts as a budget cap that limits our agent to compute features from only the most promising regions.
Notice however, that this does not imply we permit only local features, as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> also has the flexibility to incorporate global features.</p>
<p>In this work, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> is a simple mapping from patch index to patch position in the image and is a local feature.
But <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> can also be a stack of convolution layers whose receptive fields are centered at patch <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span>. If the receptive fields are large enough, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> can provide global features.</p>
<h3>Controller</h3>
<p>Temporal information between steps is important to most RL tasks, but single RGB images as our input at each time step do not provide this information.
One option is to stack multiple input frames like what is done in <dt-cite key="mnih2015humanlevel"></dt-cite>, but we find this inelegant approach unsatisfactory because the time window we care about can vary for different tasks.
Another option is to incorporate the temporal information as a hidden state inside our controller. Previous work <dt-cite key="cuccu2019playing"></dt-cite> has demonstrated that with a good representation of the input image, even a small RNN controller with only 6--18 neurons is sufficient to perform well at several Atari games using only visual inputs.</p>
<p>In our experiments, we use Long short-term memory (LSTM) <dt-cite key="hochreiter1997long"></dt-cite> network as our RNN controller so that its hidden state can capture temporal information across multiple input image frames. We find that an LSTM with only 16 neurons is sufficient to solve challenging tasks when combined with features extracted from self-attention.</p>
<h3>Neuroevolution of the Agent</h3>
<p>Operators such as importance sorting and patch pruning in our proposed methods are not gradient friendly. It is not straightforward to apply back-propagation in the learning phase.
Furthermore, restricting to gradient based learning methods can prohibit the adoption of learnable feature retrieval functions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> that consist of discrete operations or need to produce discrete features.
We therefore turn to evolution algorithms to train our agent.
While it is possible to train our agent using any evolution strategy or genetic algorithms, empirically we find the performance of Covariance Matrix Adaptation Evolution Strategy (CMA-ES) <dt-cite key="Hansen2006"></dt-cite> stable on a set of RL benchmark tasks <dt-cite key="es_on_gke"></dt-cite>.</p>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 50%;border: 1px solid transparent;"><video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/cmaes01.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
<td style="width: 50%;border: 1px solid transparent;"><video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/cmaes02.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
</tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
Visualization of CMA-ES optimization process on two function landscapes (from <dt-cite key="otoro_blog"></dt-cite>).<br/>
</figcaption>
</div>
<p>CMA-ES is an algorithm that adaptively increases or decreases the search space for the next generation given the current generation's fitness.
Concretely, CMA-ES not only adapts for the mean <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Œº</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">Œº</span></span></span></span> and standard deviation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>œÉ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">œÉ</span></span></span></span>, but also calculates the entire covariance matrix of the parameter space.
This covariance matrix essentially allows us to either explore more by increasing the variance of our search space accordingly, or fine tune the solution when the collected fitness values indicate we are close to a good optima.
However the computation of this full covariance matrix is non-trivial, and because of this CMA-ES is rarely applied to problems in high-dimensional space <dt-cite key="10.1007/978-3-319-99259-4_33"></dt-cite> such as the tasks dealing with visual inputs.
As our agent contains significantly fewer parameters than conventional methods, we are therefore able to train it with an off-the-shelf implementation of CMA-ES <dt-cite key="hansen2019pycma"></dt-cite>.</p>
<hr>
<h2>Experiments</h2>
<p>We wish to answer the following questions via experiments and analysis:</p>
<ul>
<li>
<p>Is our agent able to solve challenging vision-based RL tasks? What are the advantages over other methods that solved the same tasks?</p>
</li>
<li>
<p>How robust is the learned agent? If the agent is focusing on task-critical factors, does it generalize to the environments with modifications that are irrelevant to the core mission?</p>
</li>
</ul>
<h3>Task Description</h3>
<p>We evaluate our method in two vision-based RL tasks: CarRacing<dt-cite key="CarRacing-v0"></dt-cite> and DoomTakeCover<dt-cite key="DBLP:conf/cig/KempkaWRTJ16,DoomTakeCover-v0"></dt-cite>. The below figure are videos of our self-attention agent performing these two tasks:</p>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 60%;border: 1px solid transparent;"><video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_nomod_ours.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
<td style="width: 40%;border: 1px solid transparent;"><video src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
</tr>
</table>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 60%;border: 1px solid transparent;"><video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_nomod_ours.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
<td style="width: 40%;border: 1px solid transparent;"><video src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
</tr>
</table>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Self-attention agent playing CarRacing and DoomTakeCover</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
Left: Screenshot of the actual game environment presented to humans.<br/>Right: Resized images presented to our agent as visual input, and also its attention highlighted in white patches.<br/>
</figcaption>
</div>
<p>In CarRacing, the agent controls three continuous actions (steering left/right, acceleration and brake) of the red car to visit as many randomly generated track tiles as possible in limited steps.
At each step, the agent receives a penalty of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>‚àí</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">-0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">‚àí</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">1</span></span></span></span> but will be rewarded with a score of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>+</mo><mfrac><mrow><mn>1</mn><mn>0</mn><mn>0</mn><mn>0</mn></mrow><mrow><mi>n</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">+\frac{1000}{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="base textstyle uncramped"><span class="mord">+</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">n</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span> for every track tile it visits where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> is the total number of tiles.
Each episode ends either when all the track tiles are visited or when 1000 steps have passed.
CarRacing is considered solved if the average score over 100 consecutive test episodes is higher than 900.
Numerous works have tried to tackle this task with Deep RL algorithms,
but has not been solved until recently by methods we will refer to as <dt-cite key="ha2018worldmodels">World Model</dt-cite>, <dt-cite key="DBLP:conf/gecco/RisiS19">Genetic Algorithm (GA)</dt-cite> and <dt-cite key="DBLP:journals/corr/abs-2001-01683">Deep Innovation Protection (DIP)</dt-cite>.
For comparison purposes, we also include an implementation of Proximal Policy Optimization (PPO) <dt-cite key="schulman2017proximal,xtma_ppo"></dt-cite>, a popular RL baseline.<dt-fn>As of writing, it is difficult to find published works in the literature that have <i>solved</i> CarRacing-v0 outside of the mentioned works. After scouring online forums and GibHub, we identified four attempts. Two of these are off-policy methods that either relied on heavy <a href="https://github.com/jperod/AI-self-driving-race-car-Deep-Reinforcement-Learning/blob/master/SI_Final_Project.pdf">hand-engineering</a> to pre-process the pixels (and we were not able to reproduce by running their code), or have not actually <a href="https://github.com/AMD-RIPS/RL-2018">solved</a> it (See <a href="https://github.com/openai/gym/wiki/Leaderboard">Wiki</a>). The other two were PPO-based <a href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/CarRacing-From-Pixels-PPO">solutions</a> that relied on reward shaping to add back a penalty score upon a crash to bootstrap the agent's training, but unfortunately still used the reshaped score for evaluation. Both still scored above 800 if evaluated properly, but not above 900. Our baseline is based on the <a href="https://github.com/xtma/pytorch_car_caring">best</a> of the two PPO solutions found.</dt-fn></p>
<p>VizDoom serves as a platform for the development of agents that play DOOM using visual information. DoomTakeCover is a task in VizDoom where the agent is required to dodge the fireballs launched by the monsters and stay alive for as long as possible.
Each episode lasts for 2100 steps but ends early if the agent dies from being shot.
This is a discrete control problem where the agent can choose to move left/right or stay still at each step.
The agent gets a reward of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span> for each step it survives, and the task is regarded solved if the average accumulated reward over 100 episodes is larger than 750.
While a pre-trained <dt-cite key="ha2018worldmodels">World Model</dt-cite> is able to solve both CarRacing and this task, it has been reported that the end-to-end direct encoding <dt-cite key="DBLP:conf/gecco/RisiS19">Genetic Algorithm (GA)</dt-cite> proposed by Risi and Stanley falls short at solving this task without incorporating multi-objective optimization to preserve diversity, in <dt-cite key="DBLP:journals/corr/abs-2001-01683">Deep Innovation Protection (DIP)</dt-cite>. The PPO baseline also performed poorly on this task.<dt-fn>Despite the task's simplicity, other recent works (e.g. <a href="https://arxiv.org/abs/1912.02877">Upside Down RL</a>) also confirmed that traditional Deep RL algorithms such as DQN and A2C perform poorly on this task, perhaps due to the sparse reward signal based on survival.</dt-fn></p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/svg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/svg/agent_arch.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<b>Agent network architecture.&nbsp;</b> The numbers next to each arrow indicate data shape after the operation.
<i>A</i> is the action dimension and is task dependent. Learnable parameters in green.<br/>
</figcaption>
</div>
<p>The above figure shows our network architecture and related parameters.
We resize the input images to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>9</mn><mn>6</mn><mo>√ó</mo><mn>9</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">96 \times 96</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">9</span><span class="mord mathrm">6</span><span class="mbin">√ó</span><span class="mord mathrm">9</span><span class="mord mathrm">6</span></span></span></span> and use the same architecture for both CarRacing and DoomTakeCover (except for the output dimensions).
We use a sliding window of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">M=7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mrel">=</span><span class="mord mathrm">7</span></span></span></span> and stride <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">S=4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mrel">=</span><span class="mord mathrm">4</span></span></span></span> to segment the input image, this gives us <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>=</mo><mn>5</mn><mn>2</mn><mn>9</mn></mrow><annotation encoding="application/x-tex">N = 529</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mrel">=</span><span class="mord mathrm">5</span><span class="mord mathrm">2</span><span class="mord mathrm">9</span></span></span></span> patches.
After reshaping, we get an input matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span> of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>n</mi><mo>=</mo><mn>5</mn><mn>2</mn><mn>9</mn><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><mn>1</mn><mn>4</mn><mn>7</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(n=529, d_{in}=147)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">n</span><span class="mrel">=</span><span class="mord mathrm">5</span><span class="mord mathrm">2</span><span class="mord mathrm">9</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">4</span><span class="mord mathrm">7</span><span class="mclose">)</span></span></span></span>.
We project the input matrix to Key and Query with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">d=4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span><span class="mrel">=</span><span class="mord mathrm">4</span></span></span></span>, after self-attention is applied we extract features from the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo>=</mo><mn>1</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">K=10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span></span></span></span> most importance patches and input to the single layer LSTM controller (#hidden=16) to get the action.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/param_count.png" style="display: block; margin: auto; width: 95%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<b>Learnable parameters.&nbsp;</b> GA, DIP share the same World Model architecture. The <i>genotype</i> in our indirect encoding self-attention module are the fully connected (FC) layers that include corresponding bias terms, and the agent's controller is a small LSTM.<br/>
</figcaption>
</div>
<p>The table above summarizes the number of parameters in our agent, we have also included models from some existing works for the purpose of comparison.
For feature retrieval function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span>, we use a simple mapping from patch index to patch center position in the input image.
We normalize the positions by dividing the largest possible value so that each coordinate is between 0 and 1. In our experiments, we use pycma <dt-cite key="hansen2019pycma"></dt-cite>, an off-the-shelf implementation of CMA-ES <dt-cite key="Hansen2006"></dt-cite> to train our agent.
We use a population size of 256, set the initial standard deviation to 0.1 and keep all other parameters at default values. To deal with randomness inherent in the environments, we take the mean score over 16 rollouts in CarRacing and 5 rollouts in DoomTakeCover as the fitness of each individual in the population.</p>
<hr>
<h2>Experimental Results</h2>
<p>Not only is our agent able to solve both tasks, it also outperformed existing methods. Here is a summary of our agent's results:</p>
<div style="text-align: left;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/result_base.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<b>CarRacing and DoomTakeCover results.&nbsp;</b> We report the average score over 100 consecutive tests with standard deviations.
For reference, the required scores above which the tasks are considered solved are also included. Best scores are highlighted.<br/>
</figcaption>
</div>
<div style="text-align: left;">
<img class="b-lazy" src=data:image/svg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/svg/learning_curve.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left; padding-top: 0;">
<br/><b>Test scores vs training iterations.&nbsp;</b> We test our agent for 100 consecutive episodes with different environment seeds every 10 training iterations. The solid line shows the average score, the shaded area gives the standard deviation, and the dashed line indicates the score above which the task is considered solved.<br/>
</figcaption>
</div>
<p>In addition to the SOTA scores, the attention patches visualized in pixel space also make it easier for humans to understand the decisions made by our agent.
Here, we visualize our agent's attention by plotting the top <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> important patches elected by the self-attention module on top of the input image and see directly what the agent is attending to (the opacity indicates the importance, the whiter the more important):</p>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 50%;border: 1px solid transparent;"><video src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
<td style="width: 50%;border: 1px solid transparent;"><video src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_nomod_ours_att.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 95%;" ></video></td>
</tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
CarRacing and DoomTakeCover attention patches.<br/>
</figcaption>
</div>
<p>From visualizing the patches and observing the agent's attention, we notice that most of the patches the agent attends to are consistent with humans intuition. For example, in CarRacing, the agent's attention is on the border of the road but shifts its focus to the turns before the car needs to change its heading direction.
Notice the attentions are mostly on the left side of the road. This makes sense from a statistical point of view considering that the racing lane forms a closed loop and the car is always running in a counter-clockwise direction.</p>
<p>In DoomTakeCover, the agent is able to focus its attention on fireballs.
When the agent is near the corner of the room, it is also able to detect the wall and change its dodging strategy instead of stuck into the dead end.
Notice the agent also distributes its attention on the panel at the bottom, especially on the profile photo in the middle.
We suspect this is because the controller is using patch positions as its input, and it learned to use these points as anchors to estimate its distances to the fireballs.</p>
<p>We also notice that the scores from all methods have large variance in DoomTakeCover.
This seems to be caused by the environment‚Äôs design: some fireballs might be out of the agent‚Äôs sight but are actually approaching. The agent can still be hit by the fireballs outside its vision when it‚Äôs dodging other fireballs that are in the vision.</p>
<p>Through these tasks, we are able to give a positive answer to the first question: <em>Is our agent able to solve challenging vision-based RL tasks? What are the advantages over other methods that solved the same tasks?</em> Our agent is indeed able to solve these vision-based RL challenges. Furthermore, it is efficient in terms of being able to reach higher scores with significantly fewer parameters.</p>
<!--Furthermore, it is self-interpretable and reasons coherently as humans do because it is able to make decisions based on spatial information extracted from visual inputs.-->
<h3>Region of Interest to Importance Mapping</h3>
<p>Our feature retrieval function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> is a simple mapping from patch index to (normalized) positions of the patch's center point.
As such, this function provides information only about the locations of the patches, and discards the content inside these patches.</p>
<p>On first thought, it is actually really surprising to us that the agent is able to solve tasks with the position information alone. But after taking a closer look at the contents of the patches that the agent attends to, it is revealed that the agent learns not only <em>where</em> but also <em>what</em> to attend to. This is because the self-attention module, top <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patch selection, and the controller are all trained together as one system.</p>
<p>To illustrate this, we plot the histogram of patch importance that are in the top <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">5</span><span class="mord mathrm">%</span></span></span></span> quantile from 20 test episodes.
Although each episode presents different environmental randomness controlled by their random seeds at initialization, the distributions of the patch importance are quite consistent, this suggests our agent's behavior is coherent and stable.
When sampling and plotting patches whose importance are in the specified ranges,
we find that the agent is able to map regions of interest (ROI) to higher importance values.</p>
<div style="text-align: center;">
<a href="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/roi_histogram.png" target="_blank"><img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/roi_histogram.png" style="display: block; margin: auto; width: 100%;"/></a>
<figcaption style="text-align: left;">
<b>Region of interest to patch importance mapping (Left: CarRacing, Right: DoomTakeCover).&nbsp;</b><br/>
Importance voting mechanism via self-attention is able to identify a small minority of patches that are important for the task. The histogram shows the importance distribution of patches from 20 test episodes by patch importance scores. (Click Image to Enlarge)<br/>
</figcaption>
</div>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/png/roi_patches.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Example patches sampled from specified importance ranges from the corresponding histogram. In CarRacing, we see the most important patches are locations near the vehicle, followed by road shapes, while patches of open green fields are deemed unimporant. In DoomTakeCover, the most important patches are the fireballs.<br/>
</figcaption>
</div>
<p>The patches of the highest importance are those critical to the core mission.
These are the patches containing the red and white markers at the turns in CarRacing and the patches having fires in DoomTakeCover (patches on the left).
Shifting to the range that is around the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">5</span><span class="mord mathrm">%</span></span></span></span> quantile, the patch samples are not as interesting as before but still contains useful information such as the border of the road in CarRacing and the texture of walls in DoomTakeCover.
If we take an extreme and look at the patches with close to zero importance (patches on the right), those patches are mostly featureless and indeed have little information.</p>
<p>By mapping ROIs to importance values, the agent is able to segment and discriminate the input to its controller and learn what the objects are it is attending to. In other words, the self-attention module learns what is important to attend to and simply gives only the (normalized) <em>positions</em> of the top <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> things in the scene to the controller. As the entire system is evolved together, the controller is still able to output reasonable actions based only on position information alone.</p>
<hr>
<h2>Can our agents generalize to unseen environments?</h2>
<p>To test our agent's robustness and its ability to generalize to novel states,
we test pre-trained agents in modified versions of CarRacing and DoomTakeCover environments <em>without</em> re-training or fine-tuning them.
While there are infinitely many ways to modify an environment,
our modifications respect one important principle: the modifications should not cause changes of the core mission or critical information loss.
With this design principle in mind, we present the following modifications:</p>
<ul>
<li><strong>CarRacing--Color Perturbation¬†</strong>  We randomly perturb the background color. At the beginning of each episode, we sample two scalar perturbations uniformly from the interval <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mo>‚àí</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mo separator="true">,</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">[-0.2, 0.2]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">[</span><span class="mord">‚àí</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mclose">]</span></span></span></span> and add respectively to the lane and grass field RGB vectors. Once the perturbation is added, the colors remain constant throughout the episode.</li>
</ul>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_mod1_ours_compare.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left;">
&nbsp;Original Environment (Score: 914¬±15) vs Color Perturbation (Score: 866¬±112)<br/>
</figcaption>
<br/>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_baselines_mod1.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<table style="width: 100%;" cellspacing="0" cellpadding="0">
<tr>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;World Models (Score: 655¬±353)</figcaption></td>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;GA (Score: 442¬±362)</figcaption></td>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;PPO (Score: 505¬±464)</figcaption></td>
</tr>
</table>
</div>
<br/>
<ul>
<li><strong>CarRacing--Vertical Frames¬†</strong> We add black vertical bars to both sides of the screen. The window size of CarRacing is 800px √ó 1000px. We add two vertical bars of width 75px on the two sides of the window.</li>
</ul>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_mod2_ours_compare.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left;">
&nbsp;Original Environment (Score: 914¬±15) vs Vertical Frames (Score: 900¬±35)<br/>
</figcaption>
<br/>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_baselines_mod2.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<table style="width: 100%;" cellspacing="0" cellpadding="0">
<tr>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;World Models (Score: 166¬±137)</figcaption></td>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;GA (Score: 675¬±254)</figcaption></td>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;PPO (Score: 615¬±217)</figcaption></td>
</tr>
</table>
</div>
<br/>
<ul>
<li><strong>CarRacing--Background Blob¬†</strong> We add a red blob at a fixed position relative to the car. In CarRacing, as the lane is a closed loop and the car is designed to run in the counter clock-wise direction, the blob is placed to the north east of the car to reduce lane occlusion.</li>
</ul>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_mod3_ours_compare.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left;">
&nbsp;Original Environment (Score: 914¬±15) vs Background Blob (Score: 898¬±53)<br/>
</figcaption>
<br/>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/carracing_baselines_mod3.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<table style="width: 100%;" cellspacing="0" cellpadding="0">
<tr>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;World Models (Score: 446¬±299)</figcaption></td>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;GA (Score: 833¬±135)</figcaption></td>
<td style="width: 33.3%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;PPO (Score: 855¬±172)</figcaption></td>
</tr>
</table>
</div>
<br/>
<ul>
<li><strong>DoomTakeCover--Higher Walls¬†</strong> We make the walls higher and keep all other settings the same.</li>
</ul>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mod1_ours_compare.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Original Environment (Score: 1125¬±589) vs Higher Walls (Score: 934¬±560)<br/>
</figcaption>
<br/>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mod1_wm_all.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;"></video>
<figcaption style="text-align: left; padding-top: 0;">
World Model baseline (Score: 243¬±104)<br/>
<i>Left:</i> Rendering of modified game environment. 
<i>Center:</i> Agent's visual input. 
<i>Right:</i> Reconstruction of what the baseline agent actually sees, based on its World Model trained only on the original environment.
</figcaption>
</div>
<br/>
<ul>
<li><strong>DoomTakeCover--Different Floor Texture¬†</strong> We change the texture of the floor and keep all other settings the same.</li>
</ul>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mod2_ours_compare.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Original Environment (Score: 1125¬±589) vs Different Floor Texture (Score: 1120¬±613)<br/>
</figcaption>
<br/>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mod2_wm_all.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;"></video>
<figcaption style="text-align: left; padding-top: 0;">
World Model baseline (Score: 218¬±69)<br/>
<i>Left:</i> Rendering of modified game environment. 
<i>Center:</i> Agent's visual input. 
<i>Right:</i> Reconstruction of what the baseline agent actually sees, based on its World Model trained only on the original environment.
</figcaption>
</div>
<br/>
<ul>
<li><strong>DoomTakeCover--Hovering Text¬†</strong> We place a blue blob containing text on top part of the screen. The blob is placed to make sure no task-critical visual information is occluded.</li>
</ul>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mod3_ours_compare.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Original Environment (Score: 1125¬±589) vs Hovering Text (Score: 1035¬±627)<br/>
</figcaption>
<br/>
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/takecover_mod3_wm_all.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;"></video>
<figcaption style="text-align: left; padding-top: 0;">
World Model baseline (Score: 240¬±63)<br/>
<i>Left:</i> Rendering of modified game environment. 
<i>Center:</i> Agent's visual input. 
<i>Right:</i> Reconstruction of what the baseline agent actually sees, based on its World Model trained only on the original environment.
</figcaption>
</div>
<br/>
<p>For the purpose of comparison, we used the released code (and pre-trained models, if available) from <dt-cite key="ha2018worldmodels,DBLP:conf/gecco/RisiS19"></dt-cite> as baselines.
While our reproduced numbers do not exactly match the reported scores, they are within error bounds, and close enough for the purpose of testing for generalization.
For each modification, we test a trained agent for 100 consecutive episodes and report its scores in the following table:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/result_generalization.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
<b>Generalization Experiments&nbsp;</b> We train agents in the original task and test in the modified environments without re-training. For comparison, we also include the performance in the unmodified tasks. Results with significant performance drop highlighted.<br/>
</figcaption>
</div>
<p>Our agent generalizes well to all modifications while the baselines fail.
While World Model does not suffer a significant performance loss in color perturbations in CarRacing, it is sensitive to all changes.
Specifically, we observe <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>&gt;</mo><mn>7</mn><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">&gt; 75\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mrel">&gt;</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span><span class="mord mathrm">%</span></span></span></span> score drops in Vertical Frames, Higher Walls, Floor Texture, Hovering Text and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>&gt;</mo><mn>5</mn><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">&gt; 50\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mrel">&gt;</span><span class="mord mathrm">5</span><span class="mord mathrm">0</span><span class="mord mathrm">%</span></span></span></span> score drop in Background Blob from its performances in the unmodified tasks.</p>
<p>Since World Model's controller used as input the abstract representations it learned from reconstructing the input images, without much regularization, it is likely that the learned representations will encode visual information that is crucial to image reconstruction but not task-critical.
If this visual information to be encoded is modified in the input space, the model produces misleading representations for the controller and we see performance drop.</p>
<p>In contrast, GA and PPO performed better at generalization tests. The end-to-end training may have resulted in better task-specific representations learned compared to World model, which uses an unsupervised representation based data collected from random policies. Both GA and PPO can fine-tune their perception layers to assign greater importance to particular regions via weight learning.</p>
<p>Through these tests, we are able to answer the second question posed earlier: <em>How robust is the learned agent? If the agent is focusing on task-critical factors, does it generalize to the environments with modifications that are irrelevant to the core mission?</em></p>
<p>The small change in performance shows that our agent is robust to modifications.
Unlike baseline methods that are subject to visual distractions, our agent focuses only on task-critical positions, and simply relies on the coordinates of small patches of its visual input identified via self-attention, and is still able to keep its performance in the modified tasks without any re-training. By learning to ignore parts of the visual input that it deems irrelevant, it can naturally still perform its task even when irrelevant parts of its environment are modified.</p>
<hr>
<h2>Related Work</h2>
<p>Our work has connections to work in various areas:</p>
<p><strong>Neuroscience</strong>¬† Although the human visual processing mechanisms are not yet completely understood, recent findings from anatomical and physiological studies in monkeys suggest that visual signals are fed into processing systems to extract high level concepts such as shape, color and spatial organization <dt-cite key="freeman2011metamers,brainfacts"></dt-cite>.
Research in consciousness suggests that our brains interpret our surrounding environment in a ‚Äúlanguage of thought‚Äù that is abstract enough to interface with decision making mechanisms <dt-cite key="dehaene2014consciousness"></dt-cite>.
On the other hand, while recent works in deep RL for vision-based tasks have thrived <dt-cite key="mnih2013playing,mnih2015humanlevel"></dt-cite>, in most cases it is not clear why they work.</p>
<p>A line of work that narrows the gap is <em>World Models</em> <dt-cite key="ha2018worldmodels,hafner2018learning"></dt-cite>, where the controller's inputs are abstract representations of the visual and temporal information, provided by encouraging a probabilistic ‚ÄúWorld Model‚Äù to compress and predict potential future experiences.
Their agent excels in challenging vision-based RL tasks, and by projecting the abstract representation back to the pixel space, it is possible to get some insights into the agent's mind. <dt-cite key="DBLP:journals/corr/abs-1911-08265,gelada2019deepmdp"></dt-cite> suggest that not all details in the visual input are equally important, specifically they pointed out that rather than learning abstract representations that are capable of reconstructing the full observation, it is sufficient if the representation allows predicting quantities that are relevant for planning.</p>
<p>While we do not fully understand the mechanisms of how our brains develop abstract representations of the world, it is believed that attention is the unconscious mechanism by which we can only attend to a few selected senses at a time, allowing our consciousness to condense sensory information into a synthetic code that is compact enough to be carried forward in time for decision making <dt-cite key="vul2009attention,dehaene2014consciousness"></dt-cite>. In this work, in place of a probabilistic World Model, we investigate the use of self-attention to distill an agent's visual input into small synthetic features used as inputs for a small controller.<dt-fn>In the neuroscience and cognitive science literature, many of the descriptions of attention usually involve some sort of top-down feedback signal. The model proposed in this work is more of a bottom-up form of attention without top down feedback.</dt-fn></p>
<p><strong>Neuroevolution</strong>-based methods for tackling challenging RL tasks have recently gained popularity due to their simplicity and competitiveness to Deep RL methods, even on vision-based RL benchmark tasks <dt-cite key="salimans2017evolution,ha2017evolving,such2017deep"></dt-cite><dt-cite key="ha2018designrl,mania2018simple"></dt-cite>. More recent work <dt-cite key="DBLP:conf/gecco/RisiS19,DBLP:journals/corr/abs-2001-01683"></dt-cite> demonstrated that evolution can even train RL agents with millions of weight parameters, such as the aforementioned World Models-based agents. As these approaches do not require gradient-based computation, they offer more flexibility such as discrete latent codes, being able to optimize directly for the total reward across multiple rollouts, or ease of scaling computation across machines.</p>
<p>It is worthy to note that even before the popularity of deep RL-based approaches for vision-based tasks, indirect encoding methods from the neuroevolution literature have been used to tackle challenging vision-based tasks such as Atari domain <dt-cite key="hausknecht2012hyperneat"></dt-cite> and car navigation from pixels <dt-cite key="koutnik2013evolving"></dt-cite>. Indirect encoding methods are inspired by biological genotype--phenotype representations, and aim to represent a large, but expressive neural network with a small <em>genotype</em> code, reducing a high dimensional optimization problem to a more manageable one that can be solved with gradient-free methods.</p>
<p>Indirect encoding methods are not confined to neuroevolution. Inspired by earlier works <dt-cite key="stanley2009hyperneat,schmidhuber1993self"></dt-cite>, <em>hypernetworks</em> <dt-cite key="ha2017hypernetworks"></dt-cite> are recurrent neural networks (RNNs) whose weight matrices can change over time, depending on the RNN's input and state. It uses an outer product projection of an embedding vector, allowing a large weight matrix to be modified via a small ‚Äúgenotype‚Äù embedding. As we will show in the next section, self-attention also relies on taking an outer product of input and other parameter vectors to produce a much larger weight matrix. Transformers <dt-cite key="NIPS2017_7181"></dt-cite> demonstrated that this type of modified self-attention matrix is a tremendously powerful prior for various language modeling tasks. Here, we investigate the use of self-attention as an indirect encoding mechanism for training agents to perform vision-based RL tasks using neuroevolution.</p>
<p><strong>Attention-based RL</strong>¬† Inspired by biological vision systems, earlier works formulated the problem of visual attention as an RL problem <dt-cite key="schmidhuber1991learning,mnih2014recurrent,stollenga2014deep"></dt-cite><dt-cite key="ba2014multiple,cheung2016emergence"></dt-cite>.
Recent work <dt-cite key="zambaldi2018deep"></dt-cite> incorporated multi-head self-attention to learn representations that encode relational information between feature entities, with these features the learned agent is able to solve a novel navigation and planning task and achieve SOTA results in six out of seven StarCraft II tasks. Because the agent learned relations between entities, it can also generalize to unseen settings during training.</p>
<p>In order to capture the interactions in a system that affects the dynamics, <dt-cite key="goyal2019recurrent"></dt-cite> proposed to use a group of modified RNNs. Self-attention is used to combine their hidden states and inputs. Each member competes for attention at each step, and only the winners can access the input and also other members‚Äô states. They showed that this modular mechanism improved generalization on the Atari domain.</p>
<p>In addition to these works, attention is also explicitly used for interpretability in RL.
In <dt-cite key="Sorokin2015DeepAR"></dt-cite>, the authors incorporated soft and hard attention mechanism into the deep recurrent Q-network, and they were able to outperform Deep Q-network (DQN) <dt-cite key="mnih2013playing"></dt-cite> in a subset of the Atari games.
Most recently, <dt-cite key="DBLP:conf/nips/MottZCWR19"></dt-cite> used a soft, top-down attention mechanism to force the agent to focus on task-relevant information by sequentially querying its view of the environment.
Their agents achieved competitive performance on Atari while being more interpretable.</p>
<p>Although these methods brought exciting results, they need dedicated network architectures and carefully designed training schemes to work in an RL context.
For example, <dt-cite key="zambaldi2018deep"></dt-cite> needs to apply the self-attention mechanism iteratively within each time step to perform relational reasoning in the StarCraft II tasks.
In <dt-cite key="Sorokin2015DeepAR"></dt-cite>, a hard attention mechanism had to be separately trained because it required sampling, and in <dt-cite key="DBLP:conf/nips/MottZCWR19"></dt-cite>, a
carefully designed non-trainable basis that encodes spatial locations was needed. Because we are not concerned with gradient-based learning, we are able to chip away at the complexity and get away with using a much simplified version of the Transformer architecture in our self-attention agent. For instance, we do not even need to use positional encoding or layer normalization components in the Transformer.</p>
<p>The high dimensionality the visual input makes it computationally prohibitive to apply attention directly to individual pixels, and we rather operate on image <em>patches</em> (which have lower dimensions) instead. Although not in the context of self-attention, previous work (e.g. <dt-cite key="DBLP:conf/aaai/ChoiLZ17,DBLP:journals/corr/abs-1905-02793"></dt-cite><dt-cite key="Ding2019ImprovingSS,sumbul2019cnn"></dt-cite><dt-cite key="DBLP:journals/corr/abs-1904-01784,NIPS2019_8359"></dt-cite>) segments the visual input and attend to the patches rather than individual pixels.
Our work is similar to <dt-cite key="DBLP:conf/aaai/ChoiLZ17"></dt-cite>, where the input to the controller their is a vector of patch features weighted by attentions, the dimension of which grows linearly as we increase the number of patches.
However, as our method do not rely on gradient-based learning, we can simply restrict the input to be only the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patches with highest importance, which is an easily tunable hyper-parameter independent of patch size or patch count.
Ordinal measures have been shown <dt-cite key="rosten2005real"></dt-cite> to be robust and used in various feature detectors and descriptors in computer vision.
Using gradient-free methods (such as neuroevolution) are more desirable in the case of non-differentiable operations because these ordinal measures can be implemented as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">argmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span></span></span></span> or top <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> patch selection, critical for our self-attention agent.</p>
<hr>
<h2>Discussion</h2>
<p>While the method presented is able to cope with various out-of-domain modifications of the environment, there are limitations to this approach, and much more work to be done to further enhance the generalization capabilities of our agent. We highlight some of the limitations of the current approach in this section.</p>
<p>Much of the extra generalization capability is due to attending to the right thing, rather than from logical reasoning. For instance, if we modify the environment by adding a parallel lane next to the true lane, the agent attends to the other lane and drives there instead. Most human drivers do not drive on the opposite lane, unless they travel to another country.</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/bloopers/blooper_blue_lane_all.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Prefers to drive on the other lane</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
When we added an extra fake lane next to the real one, and set the blue channel of the color of the fake lane, the agent attends to the fake lane and proceeds to cross over and drive on it instead of the correct lane.
</figcaption>
</div>
<p>We also want to highlight that the visual module does not generalize to cases where dramatic background changes are involved.
Inspired by <dt-cite key="pineau2018,zhang2018natural"></dt-cite>, we modify the background of the car racing environment and replace the green grass background with YouTube videos <dt-cite key="catvideo2015,kof2019"></dt-cite>.</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/bloopers/blooper_cat_all.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Background suddenly becomes YouTube Cat Video <dt-cite key="catvideo2015"></dt-cite></figcaption>
<figcaption style="text-align: left; padding-top: 0;">
The agent stops to look at the cat with the fat white belly, rather than focus on the road.
</figcaption>
</div>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/bloopers/blooper_kof_all.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Background suddenly becomes The King of Fighters <dt-cite key="kof2019"></dt-cite></figcaption>
<figcaption style="text-align: left; padding-top: 0;">
The agent prefers to watch the match between the kings of fighters.
</figcaption>
</div>
<p>The agent trained on the original environment with the green grass background fails to generalize when the background is replaced with distracting YouTube videos. When we take this one step further and replace the background with pure uniform noise, we observe that the agent's attention module breaks down and attends <em>only</em> to random patches of noise, rather than to the road-related patches.</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/bloopers/noise_k10_notrain_train.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">Background is replaced with random noise</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
The agent attends only to noise, but never on the road.<br/>
<i>Left</i>: The policy trained on the normal environment does not transfer to the noisy environment (Score: -58¬±12)<br/>
<i>Right</i>: Agent trained from scratch in this noisy environment (Score: 577¬±243)
</figcaption>
</div>
<p>When we train an agent from scratch in the noisy background environment, it still manages to get around the track, although the performance is mediocre. Interestingly, the self-attention layer <em>still</em> attends only to the noise, rather than to the road, and it appears that the controller actually learns a policy to avoid such locations!</p>
<div style="text-align: center;">
<video class="b-lazy" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/mp4/bloopers/noise_k5_k20.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 97.5%;" ></video>
<table style="width: 100%;" cellspacing="0" cellpadding="0">
<tr>
<td style="width: 50%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;K=5 (Score: 452¬±210)</figcaption></td>
<td style="width: 50%;border: 1px solid transparent;"><figcaption style="text-align: left; padding-top: 0;">&nbsp;K=20 (Score: 660¬±259)</figcaption></td>
</tr>
</table>
</div>
<p>We also experiment with various <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> (the number of patches). Perhaps lowering the number will <em>force</em> the agent to focus on the road. But when we decrease <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> from 10 to 5 (or even less), the agent still attends to noisy patches rather than to the road. Not surprisingly, as we increase <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span> to 20 or even 30, the performance of this noise-avoiding policy increases.</p>
<p>These results suggest that while our current method is able to generalize to minor modifications of the environment, there is much work to be done to approach human-level generalization abilities. The simplistic choice we make to only use the patch locations (rather than their contents) may be inadequate for more complicated tasks. How we can learn more meaningful features, and perhaps even extract symbolic information from the visual input will be an exciting future direction.</p>
<hr>
<h2>Conclusion</h2>
<p>The paper demonstrated that self-attention is a powerful module for creating RL agents that is capable of solving challenging vision-based tasks.
Our agent achieves competitive results on CarRacing and DoomTakeCover with significantly fewer parameters than conventional methods, and is easily interpretable in pixel space.
Trained with neuroevolution, the agent learned to devote most of its attention to visual hints that are task-critical and is therefore able to generalize to environments where task irrelevant elements are modified while conventional methods fail.</p>
<p>Yet, our agent is nowhere close to generalization capabilities of humans. The modifications to the environments in our experiments are catered to attention-based methods. In particular, we have not modified properties of objects of interest, where our method may perform as poorly (or worse) than methods that do not require sparse attention in pixel space. We believe this work complements other approaches (e.g. <dt-cite key="kansky2017schema,higgins2017darla"></dt-cite><dt-cite key="packer2018assessing,zhao2019investigating"></dt-cite><dt-cite key="agarwal2019learning,hill2019emergent,goyal2019recurrent"></dt-cite><dt-cite key="Lee2020Network,Song2020Observational,ye2020rotation"></dt-cite>) that approach the generalization problem, and future work will continue to develop on these ideas to push the generalization abilities proposed in more general domains (such as <dt-cite key="packer2018assessing,cobbe2018quantifying"></dt-cite><dt-cite key="leibo2018psychlab,juliani2019obstacle"></dt-cite><dt-cite key="beyret2019animal,cobbe2019leveraging"></dt-cite> just to name a few).</p>
<p>Neuroevolution is a powerful toolbox for training intelligent agents, yet its adoption in RL is limited because its effectiveness when applied to large deep models was not clear until only recently <dt-cite key="such2017deep,DBLP:conf/gecco/RisiS19"></dt-cite>.
We find neuroevolution to be ideal for learning agents with self-attention.
It allows us to produce a much smaller model by removing unnecessary complexity needed for gradient-based method.
In addition, it also enables the agent to incorporate modules that include discrete and non-differentiable operations that are helpful for the tasks.
With such small yet capable models, it is exciting to see how neuroevolution trained agents would perform in vision-based tasks that are currently dominated by Deep RL algorithms in the existing literature.</p>
<p>In this work, we also establish the connections between indirect encoding methods and self-attention.
Specifically, we show that self-attention can be viewed as a form of indirect encoding.
Another interesting direction for future works is therefore to explore other forms of indirect encoding <em>bottlenecks</em> that, when combined with neuroevolution, can produce parameter efficient RL agents exhibiting interesting innate behaviors.</p>
</dt-article>
<dt-appendix>
<h2>Acknowledgements</h2>
<p>The authors would like to thank <a href="https://twitter.com/alanyttian">Yingtao Tian</a>, <a href="https://twitter.com/sina_lana">Lana Sinapayen</a>, Shixin Luo, Krzysztof Choromanski, <a href="https://twitter.com/sherjilozair">Sherjil Ozair</a>, <a href="https://twitter.com/poolio">Ben Poole</a>, <a href="https://twitter.com/kaixhin">Kai Arulkumaran</a>, <a href="https://twitter.com/ericjang11">Eric Jang</a>, <a href="https://twitter.com/thisismyhat">Brian Cheung</a>, <a href="https://twitter.com/korymath">Kory Mathewson</a>, <a href="https://twitter.com/ankurhandos">Ankur Handa</a>, and <a href="https://twitter.com/JeffDean">Jeff Dean</a> for valuable discussions.</p>
<p>Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the <a href="https://github.com/attentionagent/attentionagent.github.io/issues">discussion forum</a> for this article.</p>
<p>The experiments in this work were performed on multicore CPU Linux virtual machines provided by <a href="https://cloud.google.com/">Google Cloud Platform</a>.</p>
<div style="text-align: left;">
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/assets/icons/vision.svg" alt="Vision Icon by artist monkik on Noun Project." style="display: block; margin: auto; width: 4.5%;" align="left"/>&nbsp;&nbsp;Vision icon by artist <a href="https://thenounproject.com/kukkik_jung/">monkik</a>.
</div>
<h2 id="citation">Citation</h2>
<div style="text-align: left;">
<img src="assets/png/gecco_logo.png" alt="GECCO 2020" style="display: block; margin: auto; width: 4.5%;" align="left"/>&nbsp;&nbsp;This work will be presented at <a href="https://gecco-2020.sigevo.org/index.html/HomePage" target="_blank">GECCO 2020</a> as a full paper.
</div>
<p>For attribution in academic contexts, please cite this work as:</p>
<pre class="citation short">Yujin Tang and Duong Nguyen and David Ha, Neuroevolution of Self-Interpretable Agents, 2020.</pre>
<p>BibTeX citation</p>
<!--<pre class="citation long">@article{attentionagent2020,
  author = {Yujin Tang and Duong Nguyen and David Ha},
  title  = {Neuroevolution of Self-Interpretable Agents},
  eprint = {arXiv:2003.08165},
  url    = {https://attentionagent.github.io},
  note   = "\url{https://attentionagent.github.io}",
  year   = {2020}
}</pre>-->
<pre class="citation long">@inproceedings{attentionagent2020,
  author    = {Yujin Tang and Duong Nguyen and David Ha},
  title     = {Neuroevolution of Self-Interpretable Agents},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  url       = {https://attentionagent.github.io},
  note      = "\url{https://attentionagent.github.io}",
  year      = {2020}
}</pre>
<h2>Open Source Code</h2>
<p>Please see our <a href="https://github.com/google/brain-tokyo-workshop">repo</a> for details on reproducing the experiments in this work.</p>
<h2>Reuse</h2>
<p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a href="https://github.com/attentionagent/attentionagent.github.io">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don‚Äôt fall under this license and can be recognized by the citations in their caption.</p>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
@article {core_knowledge,
  title = {Core knowledge},
  journal = {Developmental Science},
  volume = {10},
  year = {2007},
  pages = {89-96},
  url = {http://www.wjh.harvard.edu/~lds/pdfs/SpelkeKinzler07.pdf},
  author = {Spelke, E. S and Kinzler, K. D.}
}
@article{brainfacts,
  year =         2012,
  author = {Brain Facts},
  journal = {Brain Facts},
  title =        "Vision: Processing Information",
  url =           "https://www.brainfacts.org/thinking-sensing-and-behaving/vision/2012/vision-processing-information",
  month = 4,
  lastaccessed = "January 10, 2020",
}
@article{Sorokin2015DeepAR,
  title={Deep Attention Recurrent Q-Network},
  author={Ivan Sorokin and Alexey Seleznev and Mikhail Pavlov and Aleksandr Fedorov and Anastasiia Ignateva},
  journal={ArXiv},
  year={2015},
  volume={abs/1512.01693},
  url = {https://arxiv.org/abs/1512.01693},
}
@inproceedings{DBLP:conf/nips/MottZCWR19,
  author    = {Alexander Mott and
               Daniel Zoran and
               Mike Chrzanowski and
               Daan Wierstra and
               Danilo Jimenez Rezende},
  title     = {Towards Interpretable Reinforcement Learning Using Attention Augmented
               Agents},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
               December 2019, Vancouver, BC, Canada},
  pages     = {12329--12338},
  year      = {2019},
  url       = {https://bit.ly/2Ul97za},
  timestamp = {Mon, 06 Jan 2020 18:52:01 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/nips/MottZCWR19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hausknecht2012hyperneat,
  title={HyperNEAT-GGP: A HyperNEAT-based Atari general game player},
  author={Hausknecht, Matthew and Khandelwal, Piyush and Miikkulainen, Risto and Stone, Peter},
  booktitle={Proceedings of the 14th annual conference on Genetic and evolutionary computation},
  pages={217--224},
  year={2012},
  url={http://nn.cs.utexas.edu/downloads/papers/hausknecht.gecco12.pdf},
}
@inproceedings{koutnik2013evolving,
  title={Evolving large-scale neural networks for vision-based reinforcement learning},
  author={Koutnik, Jan and Cuccu, Giuseppe and Schmidhuber, Juergen and Gomez, Faustino},
  booktitle={Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages={1061--1068},
  year={2013},
  url={http://people.idsia.ch/~juergen/compressednetworksearch.html}
}
@inproceedings{wann2019,
  title={Weight agnostic neural networks},
  author={Gaier, Adam and Ha, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5365--5379},
  year={2019},
  url = {https://weightagnostic.github.io},
}
@article{zador2019critique,
  title={A critique of pure learning and what artificial neural networks can learn from animal brains},
  author={Zador, Anthony M},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--7},
  year={2019},
  publisher={Nature Publishing Group},
  url={https://www.nature.com/articles/s41467-019-11786-6}
}
@article{stanley2009hyperneat,
  title={A hypercube-based encoding for evolving large-scale neural networks},
  author={Stanley, Kenneth O and D'Ambrosio, David B and Gauci, Jason},
  journal={Artificial life},
  volume={15},
  number={2},
  pages={185--212},
  year={2009},
  publisher={MIT Press},
  url={http://eplex.cs.ucf.edu/hyperNEATpage/}
}
@inproceedings{schmidhuber1993self,
  title={A ‚Äòself-referential‚Äô weight matrix},
  author={Schmidhuber, Juergen},
  booktitle={International Conference on Artificial Neural Networks},
  pages={446--450},
  year={1993},
  organization={Springer},
  url={https://mediatum.ub.tum.de/doc/814784/file.pdf}
}
@inproceedings{clune2009evolving,
  title={Evolving coordinated quadruped gaits with the HyperNEAT generative encoding},
  author={Clune, Jeff and Beckmann, Benjamin E and Ofria, Charles and Pennock, Robert T},
  booktitle={2009 iEEE congress on evolutionary computation},
  pages={2764--2771},
  year={2009},
  organization={IEEE},
  url={https://bit.ly/2SqUrNJ}
}
@inproceedings{risi2013confronting,
  title={Confronting the challenge of learning a flexible neural controller for a diversity of morphologies},
  author={Risi, Sebastian and Stanley, Kenneth O},
  booktitle={Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages={255--262},
  year={2013},
  url={https://eplex.cs.ucf.edu/papers/risi_gecco13b.pdf}
}
@article{stanley2007cppn,
  title={Compositional pattern producing networks: A novel abstraction of development},
  author={Stanley, Kenneth O},
  journal={Genetic programming and evolvable machines},
  volume={8},
  number={2},
  pages={131--162},
  year={2007},
  publisher={Springer},
  url={https://eplex.cs.ucf.edu/papers/stanley_gpem07.pdf}
}
@article{otorogallery,
  title={CPPN-NEAT produced artworks},
  author={David Ha},
  journal={blog.otoro.net},
  url={http://otoro.net/gallery},
  year={2015}
}
@inproceedings{opennmt,
  author    = {Guillaume Klein and
               Yoon Kim and
               Yuntian Deng and
               Jean Senellart and
               Alexander M. Rush},
  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
  booktitle = {Proc. ACL},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-4012},
  doi       = {10.18653/v1/P17-4012}
}
@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  url={https://arxiv.org/abs/1901.02860},
  year={2019}
}
@article{risi2012enhanced,
  title={An enhanced hypercube-based encoding for evolving the placement, density, and connectivity of neurons},
  author={Risi, Sebastian and Stanley, Kenneth O},
  journal={Artificial Life},
  volume={18},
  number={4},
  pages={331--363},
  year={2012},
  url={https://eplex.cs.ucf.edu/papers/risi_alife12.pdf}
}
@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}
@inproceedings{devlin-etal-2019-bert,
    title = "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  url={https://bit.ly/31PMViq}
}
@inproceedings{
DBLP:journals/corr/abs-1911-03584,
title={On the Relationship between Self-Attention and Convolutional Layers},
author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJlnC1rKPB}
}
@inproceedings{DBLP:conf/nips/ParmarRVBLS19,
  author    = {Niki Parmar and
               Prajit Ramachandran and
               Ashish Vaswani and
               Irwan Bello and
               Anselm Levskaya and
               Jon Shlens},
  title     = {Stand-Alone Self-Attention in Vision Models},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
               December 2019, Vancouver, BC, Canada},
  pages     = {68--80},
  year      = {2019},
  url       = {http://papers.nips.cc/paper/8302-stand-alone-self-attention-in-vision-models},
  timestamp = {Mon, 13 Jan 2020 09:28:31 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/nips/ParmarRVBLS19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hu2019local,
  title={Local relation networks for image recognition},
  author={Hu, Han and Zhang, Zheng and Xie, Zhenda and Lin, Stephen},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3464--3473},
  year={2019},
  url={https://arxiv.org/abs/1904.11491}
}
@inproceedings{bello2019attention,
  title={Attention augmented convolutional networks},
  author={Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3286--3295},
  year={2019},
  url={https://arxiv.org/abs/1904.09925}
}
@article{DBLP:journals/tciaig/WydmuchKJ19,
  author    = {Marek Wydmuch and
               Michal Kempka and
               Wojciech Jaskowski},
  title     = {ViZDoom Competitions: Playing Doom From Pixels},
  journal   = {{IEEE} Trans. Games},
  volume    = {11},
  number    = {3},
  pages     = {248--259},
  year      = {2019},
  doi       = {10.1109/TG.2018.2877047},
  timestamp = {Sat, 12 Oct 2019 12:50:29 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  url={https://arxiv.org/abs/1809.03470}
}
@book{dehaene2014consciousness,
  title={Consciousness and the brain: Deciphering how the brain codes our thoughts},
  author={Dehaene, Stanislas},
  year={2014},
  publisher={Penguin},
  url={https://en.wikipedia.org/wiki/Consciousness_and_the_Brain}
}
@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  url = {https://arxiv.org/abs/1312.5602},
  year={2013}
}
@article{mnih2015humanlevel,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group},
  url = {https://daiwk.github.io/assets/dqn.pdf},
}
@article{vul2009attention,
  title={Attention as inference: selection is probabilistic; responses are all-or-none samples.},
  author={Vul, Edward and Hanus, Deborah and Kanwisher, Nancy},
  journal={Journal of Experimental Psychology: General},
  volume={138},
  number={4},
  pages={546},
  year={2009},
  publisher={American Psychological Association},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822457/}
}
@article{salimans2017evolution,
  title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
 author = {Salimans, T. and Ho, J. and Chen, X. and Sidor, S. and Sutskever, I.},
journal={Preprint arXiv:1703.03864},
  year={2017},
  url={https://arxiv.org/abs/1703.03864}
}
@article{ha2017evolving,
  title={Evolving Stable Strategies},
  author = {Ha, D.},
  journal={http://blog.otoro.net/},
  year={2017},
  url="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/"
}
@article{such2017deep,
  title={Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017},
  url={https://arxiv.org/abs/1712.06567}
}
@article{ha2018designrl,
  author = {David Ha},
  title  = {Reinforcement Learning for Improving Agent Design},
  journal = {arXiv:1810.03779},
  url    = {https://designrl.github.io},
  year   = {2018}
}
@inproceedings{mania2018simple,
  title={Simple random search of static linear policies is competitive for reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1800--1809},
  year={2018},
  url={https://papers.nips.cc/paper/7451-simple-random-search-of-static-linear-policies-is-competitive-for-reinforcement-learning.pdf}
}
@inproceedings{ha2017hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  booktitle={Fifth International Conference on Learning Representations (ICLR 2017)},
  url={https://openreview.net/forum?id=rkpACe1lx},
  year={2017}
}
@article{schmidhuber1991learning,
  title={Learning to generate artificial fovea trajectories for target detection},
  author={Schmidhuber, Juergen and Huber, Rudolf},
  journal={International Journal of Neural Systems},
  volume={2},
  number={01n02},
  pages={125--134},
  year={1991},
  publisher={World Scientific},
  url={http://people.idsia.ch/~juergen/attentive.html}
}
@inproceedings{stollenga2014deep,
  title={Deep networks with internal selective attention through feedback connections},
  author={Stollenga, Marijn F and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},
  booktitle={Advances in neural information processing systems},
  pages={3545--3553},
  year={2014},
  url={https://arxiv.org/abs/1407.3068}
}
@inproceedings{
zambaldi2018deep,
title={Deep reinforcement learning with relational inductive biases},
author={Vinicius Zambaldi and David Raposo and Adam Santoro and Victor Bapst and Yujia Li and Igor Babuschkin and Karl Tuyls and David Reichert and Timothy Lillicrap and Edward Lockhart and Murray Shanahan and Victoria Langston and Razvan Pascanu and Matthew Botvinick and Oriol Vinyals and Peter Battaglia},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HkxaFoC9KQ},
}
@article{Sorokin2015DeepAR,
  title={Deep Attention Recurrent Q-Network},
  author={Ivan Sorokin and Alexey Seleznev and Mikhail Pavlov and Aleksandr Fedorov and Anastasiia Ignateva},
  journal={ArXiv},
  year={2015},
  volume={abs/1512.01693},
  url={https://arxiv.org/abs/1512.01693}
}
@inproceedings{DBLP:conf/aaai/ChoiLZ17,
  author    = {Jinyoung Choi and
               Beom{-}Jin Lee and
               Byoung{-}Tak Zhang},
  title     = {Multi-Focus Attention Network for Efficient Deep Reinforcement Learning},
  booktitle = {The Workshops of the The Thirty-First {AAAI} Conference on Artificial
               Intelligence, Saturday, February 4-9, 2017, San Francisco, California,
               {USA}},
  year      = {2017},
  url       = {http://aaai.org/ocs/index.php/WS/AAAIW17/paper/view/15100},
  timestamp = {Wed, 24 Apr 2019 10:44:20 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/ChoiLZ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{bloem2018,
  title={Transformers from Scratch},
  author = {Peter Bloem},
  journal={http://www.peterbloem.nl/},
  year={2019},
  url="http://www.peterbloem.nl/blog/transformers"
}
@inproceedings{munkhdalai2017meta,
  title={Meta networks},
  author={Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2554--2563},
  year={2017},
  organization={JMLR. org},
  url={https://arxiv.org/abs/1703.00837}
}
@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in neural information processing systems},
  pages={6571--6583},
  year={2018},
  url={https://arxiv.org/abs/1806.07366}
}
@article{dumoulin2018featurewise,
  author = {Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm de and Courville, Aaron and Bengio, Yoshua},
  title = {Feature-wise transformations},
  journal = {Distill},
  year = {2018},
  url = {https://distill.pub/2018/feature-wise-transformations},
  doi = {10.23915/distill.00011}
}
@inproceedings{Oswald2020Continual,
title={Continual learning with hypernetworks},
author={Johannes von Oswald and Christian Henning and Jo√£o Sacramento and Benjamin F. Grewe},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgwNerKvB}
}
@inproceedings{ba2016using,
  title={Using fast weights to attend to the recent past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4331--4339},
  year={2016},
  url={https://arxiv.org/abs/1610.06258}
}
@article{miconi2018differentiable,
  title={Differentiable plasticity: training plastic neural networks with backpropagation},
  author={Miconi, Thomas and Clune, Jeff and Stanley, Kenneth O},
  journal={arXiv preprint arXiv:1804.02464},
  year={2018},
  url={https://arxiv.org/abs/1804.02464}
}
@book{hebb1949organization,
  title={The organization of behavior},
  author={Hebb, Donald O},
  year={1949},
  publisher={na},
  url={https://en.wikipedia.org/wiki/Hebbian_theory}
}
@article{koren2009matrix,
  title={Matrix factorization techniques for recommender systems},
  author={Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  journal={Computer},
  volume={42},
  number={8},
  pages={30--37},
  year={2009},
  publisher={IEEE},
  url={https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf}
}
@online{pineau2018,
  author = {Joelle Pineau},
  title = {Reproducible, Reusable, and Robust Reinforcement Learning - NeurIPS 2018},
  year = 2018,
  url = {https://youtu.be/Kee4ch3miVA?t=2077},
  urldate = {2020-2-1}
}
@online{catvideo2015,
  author = {Extraordinary Epic},
  title = {Fat Cat Fail Vines Compilation},
  publisher = {Video from YouTube (Creative Commons license, reuse allowed)},
  year = 2015,
  url = "https://youtu.be/zeeH-Z_Y4as",
  urldate = {2020-3-1}
}
@online{kof2019,
  author = {Satsui Hado},
  title = {The King of Fighters '98 Arcade Playthrough},
  year = 2019,
  publisher = {The King of Fighters is a registered trademark of SNK Playmore Corporation.<br/>Video from YouTube (Creative Commons license, reuse allowed)},
  url = {https://youtu.be/Q3fpMQ3uqBQ},
  urldate = {2020-3-1}
}
@article{zhang2018natural,
  title={Natural environment benchmarks for reinforcement learning},
  author={Zhang, Amy and Wu, Yuxin and Pineau, Joelle},
  journal={arXiv preprint arXiv:1811.06032},
  year={2018},
  url={https://arxiv.org/abs/1811.06032}
}
@book{mack1998inattentional,
  title={Inattentional blindness},
  author={Mack, Arien and Rock, Irvin and others},
  year={1998},
  publisher={MIT press},
  url={https://en.wikipedia.org/wiki/Inattentional_blindness}
}
@book{kahneman2011thinking,
  author = {Kahneman, Daniel},
  publisher = {Farrar, Straus and Giroux},
  title = {Thinking, Fast and Slow},
  url = {https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow},
  year = {2011}
}
@inproceedings{cuccu2019playing,
  title={Playing atari with six neurons},
  author={Cuccu, Giuseppe and Togelius, Julian and Cudre-Mauroux, Philippe},
  booktitle={Proceedings of the 18th international conference on autonomous agents and multiagent systems},
  pages={998--1006},
  year={2019},
  organization={International Foundation for Autonomous Agents and Multiagent Systems},
  url={https://arxiv.org/abs/1806.01363}
}
@article{hochreiter1997long,
  author = {Hochreiter, Sepp and Schmidhuber, Juergen},
  journal = {Neural computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  title = {Long short-term memory},
  volume = 9,
  year = {1997},
  url = {http://people.idsia.ch/~juergen/rnn.html}
}
@incollection{Hansen2006,
  title={The CMA evolution strategy: a comparing review},
  author={Hansen, Nikolaus},
  booktitle={Towards a new evolutionary computation},
  pages={75--102},
  year={2006},
  publisher={Springer},
  url={https://bit.ly/39TxcBR}
}
@article{es_on_gke,
  title={How to run evolution strategies on Google Kubernetes Engine},
  author = {Yujin Tang and David Ha},
  journal={https://cloud.google.com/blog},
  year={2019},
  url="https://cloud.google.com/blog/products/ai-machine-learning/how-to-run-evolution-strategies-on-google-kubernetes-engine"
}
@article{otoro_blog,
  title={A Visual Guide to Evolution Strategies},
  author = {David Ha},
  journal={http://blog.otoro.net},
  year={2017},
  url="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/"
}
@inproceedings{10.1007/978-3-319-99259-4_33,
  title={Challenges in high-dimensional reinforcement learning with evolution strategies},
  author={Muller, Nils and Glasmachers, Tobias},
  booktitle={International Conference on Parallel Problem Solving from Nature},
  pages={411--423},
  year={2018},
  organization={Springer},
  url={https://arxiv.org/abs/1806.01224}
}
@misc{hansen2019pycma,
  author       = {Nikolaus Hansen and Youhei Akimoto and Petr Baudis},
  title        = {CMA-ES/pycma on Github},
  howpublished = {Zenodo, DOI:10.5281/zenodo.2559634},
  month        = feb,
  year         = {2019},
  doi          = {10.5281/zenodo.2559634},
  url          = {https://doi.org/10.5281/zenodo.2559634},
}
@ARTICLE{brockman2016openai,
  author = {Brockman, G. and Cheung, V. and Pettersson, L. and Schneider, J. and Schulman, J. and Tang, J. and Zaremba, W.},
  title={OpenAI Gym},
  journal={Preprint arXiv:1606.01540},
  year = 2016,
  month = jun,
  url = {https://arxiv.org/abs/1606.01540},
}
@online{CarRacing-v0,
  year = {2016},
  author = {Oleg Klimov},
  title =        "CarRacing-v0",
  url =           "https://gym.openai.com/envs/CarRacing-v0/",
  lastaccessed = "January 17, 2020",
}
@online{DoomTakeCover-v0,
  year = {2017},
  author = {Philip Paquette},
  title =        "DoomTakeCover-v0",
  url =           "https://gym.openai.com/envs/DoomTakeCover-v0/",
  lastaccessed = "January 17, 2020",
}
@inproceedings{DBLP:conf/cig/KempkaWRTJ16,
  author    = {Michal Kempka and
               Marek Wydmuch and
               Grzegorz Runc and
               Jakub Toczek and
               Wojciech Jaskowski},
  title     = {ViZDoom: A Doom-based AI research platform for visual reinforcement
               learning},
  booktitle = {IEEE Conference on Computational Intelligence and Games, CIG 2016,
               Santorini, Greece, September 20-23, 2016},
  pages     = {1--8},
  year      = {2016},
  url       = {https://doi.org/10.1109/CIG.2016.7860433},
  doi       = {10.1109/CIG.2016.7860433},
  timestamp = {Wed, 16 Oct 2019 14:14:56 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/cig/KempkaWRTJ16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{ha2018worldmodels,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, Juergen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages = {2451--2463},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  url = {https://worldmodels.github.io},
}
@article{hafner2018learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  journal={arXiv preprint arXiv:1811.04551},
  url={https://planetrl.github.io/},
  year={2018}
}
@article{kaiser2019model,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019},
  url = {https://arxiv.org/abs/1903.00374},
}
@inproceedings{freeman2019learning,
  title={Learning to Predict Without Looking Ahead: World Models Without Forward Prediction},
  author={Freeman, Daniel and Ha, David and Metz, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5380--5391},
  year={2019},
  url={https://learningtopredict.github.io/}
}
@article{DBLP:journals/corr/abs-1911-08265,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal   = {CoRR},
  volume    = {abs/1911.08265},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.08265},
  archivePrefix = {arXiv},
  eprint    = {1911.08265},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{gelada2019deepmdp,
  title={Deepmdp: Learning continuous latent space models for representation learning},
  author={Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum, Ofir and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1906.02736},
  year={2019},
  url={https://arxiv.org/abs/1906.02736}
}
@inproceedings{DBLP:conf/gecco/RisiS19,
  author    = {Sebastian Risi and
               Kenneth O. Stanley},
  title     = {Deep neuroevolution of recurrent and discrete world models},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference,
               GECCO 2019, Prague, Czech Republic, July 13-17, 2019},
  pages     = {456--462},
  year      = {2019},
  url       = {https://arxiv.org/abs/1906.08857},
  doi       = {10.1145/3321707.3321817},
  timestamp = {Thu, 04 Jul 2019 12:06:53 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2001-01683,
  author    = {Sebastian Risi and
               Kenneth O. Stanley},
  title     = {Improving Deep Neuroevolution via Deep Innovation Protection},
  journal   = {CoRR},
  volume    = {abs/2001.01683},
  year      = {2020},
  url       = {http://arxiv.org/abs/2001.01683},
  archivePrefix = {arXiv},
  eprint    = {2001.01683},
  timestamp = {Fri, 10 Jan 2020 13:10:19 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{such2017deep,
  title={Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  url       = {http://arxiv.org/abs/1712.06567},
  year={2017}
}
@inproceedings{mnih2014recurrent,
  title={Recurrent models of visual attention},
  author={Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and others},
  booktitle={Advances in neural information processing systems},
  pages={2204--2212},
  year={2014},
  url={https://arxiv.org/abs/1406.6247}
}
@article{ye2020rotation,
  title={Rotation, Translation, and Cropping for Zero-Shot Generalization},
  author={Ye, Chang and Khalifa, Ahmed and Bontrager, Philip and Togelius, Julian},
  journal={arXiv preprint arXiv:2001.09908},
  year={2020},
  url={https://arxiv.org/abs/2001.09908}
}
@article{zhao2019investigating,
  title={Investigating generalisation in continuous deep reinforcement learning},
  author={Zhao, Chenyang and Siguad, Olivier and Stulp, Freek and Hospedales, Timothy M},
  journal={arXiv preprint arXiv:1902.07015},
  year={2019},
  url={https://arxiv.org/abs/1902.07015}
}
@article{packer2018assessing,
  title={Assessing generalization in deep reinforcement learning},
  author={Packer, Charles and Gao, Katelyn and Kos, Jernej and Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen and Song, Dawn},
  journal={arXiv preprint arXiv:1810.12282},
  year={2018},
  url={https://arxiv.org/abs/1810.12282}
}
@article{agarwal2019learning,
  title={Learning to generalize from sparse and underspecified rewards},
  author={Agarwal, Rishabh and Liang, Chen and Schuurmans, Dale and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1902.07198},
  year={2019},
  url={https://arxiv.org/abs/1902.07198}
}
@article{hill2019emergent,
  title={Emergent systematic generalization in a situated agent},
  author={Hill, Felix and Lampinen, Andrew and Schneider, Rosalia and Clark, Stephen and Botvinick, Matthew and McClelland, James L and Santoro, Adam},
  journal={arXiv preprint arXiv:1910.00571},
  year={2019},
  url={https://arxiv.org/abs/1910.00571}
}
@article{cobbe2018quantifying,
  title={Quantifying generalization in reinforcement learning},
  author={Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  journal={arXiv preprint arXiv:1812.02341},
  year={2018},
  url={https://arxiv.org/abs/1812.02341}
}
@article{juliani2019obstacle,
  title={Obstacle tower: A generalization challenge in vision, control, and planning},
  author={Juliani, Arthur and Khalifa, Ahmed and Berges, Vincent-Pierre and Harper, Jonathan and Teng, Ervin and Henry, Hunter and Crespi, Adam and Togelius, Julian and Lange, Danny},
  journal={arXiv preprint arXiv:1902.01378},
  year={2019},
  url={https://arxiv.org/abs/1902.01378}
}
@inproceedings{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9505--9515},
  year={2018},
  url={https://arxiv.org/abs/1810.03292}
}
@article{Ding2019ImprovingSS,
  title={Improving Semantic Segmentation of Aerial Images Using Patch-based Attention},
  author={Lei Ding and Hao Tang and Lorenzo Bruzzone},
  journal={ArXiv},
  year={2019},
  url={https://arxiv.org/abs/1911.08877}
}
@article{DBLP:journals/corr/abs-1904-01784,
  author    = {Yuning Chai},
  title     = {Patchwork: A Patch-wise Attention Network for Efficient Object Detection
               and Segmentation in Video Streams},
  journal   = {CoRR},
  volume    = {abs/1904.01784},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.01784},
  archivePrefix = {arXiv},
  eprint    = {1904.01784},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-01784.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{sumbul2019cnn,
  title={A CNN-RNN Framework with a Novel Patch-Based Multi-Attention Mechanism for Multi-Label Image Classification in Remote Sensing},
  author={Sumbul, Gencer and Demir, Begum},
  journal={arXiv preprint arXiv:1902.11274},
  year={2019},
  url = {https://arxiv.org/abs/1902.11274}
}
@article{DBLP:journals/corr/abs-1905-02793,
  title={Skin Lesion Classification Using CNNs with Patch-Based Attention and Diagnosis-Guided Loss Weighting},
  author={Gessert, Nils and Sentker, Thilo and Madesta, Frederic and Schmitz, Rudiger and Kniep, Helge and Baltruschat, Ivo and Werner, Rene and Schlaefer, Alexander},
  journal={IEEE Transactions on Biomedical Engineering},
  year={2019},
  publisher={IEEE},
  url={https://arxiv.org/abs/1905.02793}
}
@incollection{NIPS2019_8359,
  title={Saccader: Improving Accuracy of Hard Attention Models for Vision},
  author={Elsayed, Gamaleldin and Kornblith, Simon and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={700--712},
  year={2019},
  url = {https://arxiv.org/abs/1908.07644}
}
@article{stanley2003taxonomy,
  title={A taxonomy for artificial embryogeny},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Artificial Life},
  volume={9},
  number={2},
  pages={93--130},
  year={2003},
  publisher={MIT Press},
  url={http://nn.cs.utexas.edu/?stanley:alife03}
}
@article{clune2011performance,
  title={On the performance of indirect encoding across the continuum of regularity},
  author={Clune, Jeff and Stanley, Kenneth O and Pennock, Robert T and Ofria, Charles},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={15},
  number={3},
  pages={346--367},
  year={2011},
  publisher={IEEE},
  url={https://bit.ly/2V8g3QG}
}
@article{schmidhuber1997discovering,
  title={Discovering neural nets with low Kolmogorov complexity and high generalization capability},
  author={Schmidhuber, Juergen},
  journal={Neural Networks},
  volume={10},
  number={5},
  pages={857--873},
  year={1997},
  publisher={Elsevier},
  url={ftp://ftp.idsia.ch/pub/juergen/loconet.pdf}
}
@inproceedings{gauci2008case,
  title={A Case Study on the Critical Role of Geometric Regularity in Machine Learning.},
  author={Gauci, Jason and Stanley, Kenneth O},
  booktitle={AAAI},
  pages={628--633},
  year={2008},
  url={https://www.aaai.org/Papers/AAAI/2008/AAAI08-100.pdf}
}
@inproceedings{gauci2010indirect,
  title={Indirect encoding of neural networks for scalable Go},
  author={Gauci, Jason and Stanley, Kenneth O},
  booktitle={International Conference on Parallel Problem Solving from Nature},
  pages={354--363},
  year={2010},
  organization={Springer},
  url={http://eplex.cs.ucf.edu/papers/gauci_ppsn10.pdf}
}
@inproceedings{finn2016deep,
  title={Deep spatial autoencoders for visuomotor learning},
  author={Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={512--519},
  year={2016},
  organization={IEEE},
  url={https://arxiv.org/abs/1509.06113}
}
@article{ramanujan2019s,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:1911.13299},
  year={2019},
  url={https://arxiv.org/abs/1911.13299}
}
@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}
@article{zhou2019deconstructing,
  title={Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  journal={arXiv preprint arXiv:1905.01067},
  url={https://arxiv.org/abs/1905.01067},
  year={2019}
}
@article{hasson2020direct,
  title={Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks},
  author={Hasson, Uri and Nastase, Samuel A and Goldstein, Ariel},
  journal={Neuron},
  volume={105},
  number={3},
  pages={416--434},
  year={2020},
  publisher={Elsevier},
  url={https://www.biorxiv.org/content/10.1101/764258v2.full}
}
@inproceedings{krizhevsky2012imagenet,
  title={ImageNet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012},
  url={https://bit.ly/2w3DcZM}
}
@article{cheung2016emergence,
  title={Emergence of foveal image sampling from learning to attend in visual scenes},
  author={Cheung, Brian and Weiss, Eric and Olshausen, Bruno},
  journal={arXiv preprint arXiv:1611.09430},
  year={2016},
  url={https://arxiv.org/abs/1611.09430},
}
@article{ba2014multiple,
  title={Multiple object recognition with visual attention},
  author={Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1412.7755},
  year={2014},
  url={https://arxiv.org/abs/1412.7755}
}
@inproceedings{kansky2017schema,
  title={Schema networks: Zero-shot transfer with a generative causal model of intuitive physics},
  author={Kansky, Ken and Silver, Tom and M{\'e}ly, David A and Eldawy, Mohamed and L{\'a}zaro-Gredilla, Miguel and Lou, Xinghua and Dorfman, Nimrod and Sidor, Szymon and Phoenix, Scott and George, Dileep},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1809--1818},
  year={2017},
  organization={JMLR. org},
  url={https://arxiv.org/abs/1706.04317}
}
@article{rosten2008faster,
  title={Faster and better: A machine learning approach to corner detection},
  author={Rosten, Edward and Porter, Reid and Drummond, Tom},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={32},
  number={1},
  pages={105--119},
  year={2008},
  publisher={IEEE},
  url={https://arxiv.org/abs/0810.2434}
}
@inproceedings{kulkarni2019unsupervised,
  title={Unsupervised learning of object keypoints for perception and control},
  author={Kulkarni, Tejas D and Gupta, Ankush and Ionescu, Catalin and Borgeaud, Sebastian and Reynolds, Malcolm and Zisserman, Andrew and Mnih, Volodymyr},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10723--10733},
  year={2019},
  url={https://bit.ly/2wLiEFZ}
}
@article{leibo2018psychlab,
  title={Psychlab: a psychology laboratory for deep reinforcement learning agents},
  author={Leibo, Joel Z and d'Autume, Cyprien de Masson and Zoran, Daniel and Amos, David and Beattie, Charles and Anderson, Keith and Casta{\~n}eda, Antonio Garc{\'\i}a and Sanchez, Manuel and Green, Simon and Gruslys, Audrunas and others},
  journal={arXiv preprint arXiv:1801.08116},
  year={2018},
  url={https://arxiv.org/abs/1801.08116}
}
@article{beyret2019animal,
  title={The Animal-AI Environment: Training and Testing Animal-Like Artificial Cognition},
  author={Beyret, Benjamin and Hern{\'a}ndez-Orallo, Jos{\'e} and Cheke, Lucy and Halina, Marta and Shanahan, Murray and Crosby, Matthew},
  journal={arXiv preprint arXiv:1909.07483},
  year={2019},
  url={https://arxiv.org/abs/1909.07483}
}
@inproceedings{sainath2013low,
  title={Low-rank matrix factorization for deep neural network training with high-dimensional output targets},
  author={Sainath, Tara N and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6655--6659},
  year={2013},
  organization={IEEE},
  url={https://bit.ly/39ZEF26}
}
@inproceedings{grosse2016kronecker,
  title={A kronecker-factored approximate fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016},
  url={http://www.jmlr.org/proceedings/papers/v48/grosse16.pdf}
}
@inproceedings{suwajanakorn2018discovery,
  title={Discovery of latent 3d keypoints via end-to-end geometric reasoning},
  author={Suwajanakorn, Supasorn and Snavely, Noah and Tompson, Jonathan J and Norouzi, Mohammad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2059--2070},
  year={2018},
  url={https://keypointnet.github.io/}
}
@inproceedings{
Lee2020Network,
title={Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning},
author={Kimin Lee and Kibok Lee and Jinwoo Shin and Honglak Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJgcvJBFvB}
}
@inproceedings{
Song2020Observational,
title={Observational Overfitting in Reinforcement Learning},
author={Xingyou Song and Yiding Jiang and Stephen Tu and Yilun Du and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJli2hNKDH}
}
@article{cobbe2019leveraging,
  title={Leveraging Procedural Generation to Benchmark Reinforcement Learning},
  author={Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
  journal={arXiv preprint arXiv:1912.01588},
  year={2019},
  url={https://arxiv.org/abs/1912.01588}
}
@inproceedings{higgins2017darla,
  title={Darla: Improving zero-shot transfer in reinforcement learning},
  author={Higgins, Irina and Pal, Arka and Rusu, Andrei and Matthey, Loic and Burgess, Christopher and Pritzel, Alexander and Botvinick, Matthew and Blundell, Charles and Lerchner, Alexander},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1480--1490},
  year={2017},
  organization={JMLR. org},
  url={https://arxiv.org/abs/1707.08475}
}
@inproceedings{rosten2005real,
  title={Real-time video annotations for augmented reality},
  author={Rosten, Edward and Reitmayr, Gerhard and Drummond, Tom},
  booktitle={International Symposium on Visual Computing},
  pages={294--302},
  year={2005},
  organization={Springer},
  url={http://www.edrosten.com/work/rosten_2005_annotations.pdf}
}
@article{freeman2011metamers,
  title={Metamers of the ventral stream},
  author={Freeman, Jeremy and Simoncelli, Eero P},
  journal={Nature neuroscience},
  volume={14},
  number={9},
  pages={1195},
  year={2011},
  publisher={Nature Publishing Group},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3164938/}
}
@online{xtma_ppo,
  year = {2019},
  title = {Car Racing with PyTorch},
  author= {Xiaoteng Ma},
  url = {https://github.com/xtma/pytorch_car_caring},
  lastaccessed = "March 6, 2020",
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://arxiv.org/abs/1707.06347}
}
@article{goyal2019recurrent,
  title={Recurrent independent mechanisms},
  author={Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1909.10893},
  url={https://arxiv.org/abs/1909.10893},
  year={2019}
}
</script>
<script src="https://storage.googleapis.com/quickdraw-models/sketchRNN/attention/lib/blazy.js"></script>
<script>
  // blazy code
  var bLazy = new Blazy({
    success: function(){
      updateCounter();
    }
  });

  // not needed, only here to illustrate amount of loaded images
  var imageLoaded = 0;

  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
</script>